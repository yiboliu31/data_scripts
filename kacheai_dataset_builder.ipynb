{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import hashlib\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "import time\n",
    "import pickle\n",
    "from utils import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "HEADER_ROW=['bag', 'time_sec', 'time_nsec', 'GPS', 'v_ego', 'key_event', 'frame_name']\n",
    "frames_dir = '/media/dean/deans_data/construction_frames/'\n",
    "bag_dirs = os.listdir(frames_dir)\n",
    "save_dir = os.path.join(frames_dir,'kache_set')\n",
    "os.makedirs(save_dir, 0o755 , exist_ok = True )\n",
    "\n",
    "img_data = []\n",
    "lookup_table = {}\n",
    "\n",
    "for bag in bag_dirs:\n",
    "    if os.path.join(frames_dir,bag) != save_dir:\n",
    "        img_dict = {}\n",
    "        bag_path = os.path.join(frames_dir, bag)\n",
    "        img_dict['bag_path'] = bag_path\n",
    "        img_dict['frames'] = []\n",
    "        img_dict['csv_path'] = os.path.join(bag_path, 'frames.csv')\n",
    "        csv_logger = pd.read_csv(img_dict['csv_path'], names=HEADER_ROW, skiprows=1)\n",
    "\n",
    "        # Get all images\n",
    "        imgs = glob.glob(os.path.join(bag_path,'**/', '*.jpg'))\n",
    "\n",
    "        for img in imgs:\n",
    "            d = {}\n",
    "            d['frame_path'] = img\n",
    "            new_path = os.path.split(os.path.split(os.path.split(img)[0])[0])[1].rstrip('/')+'-'+os.path.split(img)[1]\n",
    "            hash_object = hashlib.sha1(str.encode(new_path))\n",
    "            hex_dig = hash_object.hexdigest()\n",
    "            #shutil.copyfile(img, os.path.join(save_dir, str(hex_dig)+'.jpg'))\n",
    "            d['dataset_path'] = os.path.join(save_dir, str(hex_dig)+'.jpg')\n",
    "            d['hash_key'] = hex_dig\n",
    "            d['hash_val'] = new_path\n",
    "            lookup_table[d['hash_key']] = d['hash_val'] \n",
    "            old_path = os.path.join(bag_path, 'frames', d['frame_path'].split('-')[-1])\n",
    "\n",
    "            for log in [x for x in csv_logger.as_matrix() if os.path.join(bag_path, 'frames', x[6].rstrip('/')) == old_path.rstrip('/')]:\n",
    "                d['bag_name'] = log[0]\n",
    "                d['time_sec'] = log[1]\n",
    "                d['time_readable'] = time.ctime(int(log[1]))\n",
    "\n",
    "                d['time_nsec'] = log[2]\n",
    "                d['GPS'] = log[3]\n",
    "                d['v-ego'] = log[4]\n",
    "                                    \n",
    "                d['key_event'] = log[5]\n",
    "                d['frame'] = log[6]\n",
    "\n",
    "            if d.get('frame', None): img_dict['frames'].append(d)\n",
    "        if img_dict['frames']: img_data.append((bag_path,img_dict))\n",
    "\n",
    "pickle_file = '/media/dean/datastore/datasets/kache_ai/frames/kacheai_logs.pickle'\n",
    "pickle_dict = {'img_data':img_data,'lookup_table':lookup_table}\n",
    "print('Saving to Pickle File:', pickle_file)\n",
    "with open(pickle_file,\"wb\") as pickle_out:\n",
    "    pickle.dump(pickle_dict, pickle_out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(img_data[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Annotation Pipeline via Darknet ... \\o/ \\o/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "WORKING_DIR = '/media/dean/datastore/datasets/kache_ai'\n",
    "kache_set = datasets.DataFormatter(annotations_list = '/media/dean/datastore/datasets/kache_ai/frames/kacheai_logs.pickle', image_list = '/media/dean/datastore/datasets/kache_ai/frames/', input_format = datasets.Format.kache,\n",
    "                                 output_path = os.path.join(WORKING_DIR,'scalabel_files'),\n",
    "                                 trainer_prefix = 'COCO_train2014_0000', \n",
    "                                 s3_bucket = 'kache-scalabel/kache_ai/frames/', check_s3 = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kache_set.export(format = datasets.Format.scalabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 86469: 4.864921, 4.839282 avg loss, 0.000080 rate, 13.939101 seconds, 177088512 images\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
