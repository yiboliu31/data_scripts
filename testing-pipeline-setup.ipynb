{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights Config:, {'weights_path': '/media/dean/datastore/perception-pipelines/pipelines/models/1557540719/weights/weights_1000.pt', 'labels': ['construct-cone', 'construct-sign', 'construct-barrel', 'construct-pole', 'construct-post', 'construct-equipment'], 'anchors': [[3, 5], [5, 10], [10, 17], [16, 27], [24, 43], [33, 65], [39, 100], [72, 60], [76, 177]], 'job_id': 'AQEB74DxvF3Cu/CUgedqsq92cD78eAUroYuzTZxoHPmXmy4CaPEJxj2pidwSysKEZ/DXbrDhy5/DWrEtMVszG3oohs8wyjiz9DqjO6MYDOly6sin03fTT8C9Gx7Oag+ZuEJq5hy7o07Ygxw6/cUdkHzCId/HBGLQ+ZvYFTT/fNu1yI2wBcq9F7uAE/1KcxHtm27xw54tr5aabkq7hiRXt2zSh6ehQUnowLabmuhut4gUnQ4R7b49s4vjIIOt403ZOb6aJPnmAwGffDjkRu0JjHLnV3tEQyQc/IXcqHEyaidXBQc='}\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import os\n",
    "import requests\n",
    "import toml\n",
    "import ast\n",
    "from ast import literal_eval as make_tuple\n",
    "import json\n",
    "import boto3\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "import shutil\n",
    "\n",
    "\n",
    "# Get the service resource\n",
    "sqs = boto3.resource('sqs')\n",
    "\n",
    "# Get the queue\n",
    "queue = sqs.get_queue_by_name(QueueName='model_testing.fifo')\n",
    "weights_config = None\n",
    "# Process messages by printing out body\n",
    "for message in queue.receive_messages(MaxNumberOfMessages=1):\n",
    "    msg_dict = ast.literal_eval(message.body)\n",
    "\n",
    "weights_config = msg_dict\n",
    "weights_config['job_id'] = message.receipt_handle\n",
    "\n",
    "# Let the queue know that the message is processed\n",
    "# message.delete()\n",
    "\n",
    "# Print out the body of the message\n",
    "print('Weights Config:, {0}'.format(weights_config))\n",
    "\n",
    "    \n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kachedb_auth_token(kache_db_username, kache_db_password):\n",
    "    base_auth_url = kache_db_api_url + 'auth/login'\n",
    "    body = {\"email\": kache_db_username,\n",
    "            \"password\": kache_db_password,\n",
    "            }\n",
    "    try:\n",
    "        resp = requests.post(base_auth_url, json=body)\n",
    "    except requests.exceptions.Timeout as e1:\n",
    "        # Maybe set up for a retry, or continue in a retry loop\n",
    "        print(e1)\n",
    "    except requests.exceptions.TooManyRedirects as e2:\n",
    "        # Tell the user their URL was bad and try a different one\n",
    "        print(e2)\n",
    "    except requests.exceptions.RequestException as e3:\n",
    "        # catastrophic error. bail.\n",
    "        print(e3)\n",
    "        sys.exit(1)\n",
    "\n",
    "    if resp.status_code != 200:\n",
    "        raise requests.exceptions.RequestException(\n",
    "            f'POST /create status code: {resp.status_code}, content {resp.content}')\n",
    "\n",
    "    content_data = json.loads(resp.content)\n",
    "    kache_db_auth_token = content_data['auth_token']\n",
    "\n",
    "    return kache_db_auth_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_traffic_light_cats(data):\n",
    "    for img_data in data:\n",
    "        if img_data.get('labels', None):\n",
    "            for label in [l for l in  img_data['labels'] if l['category'] == 'traffic light' and l.get('attributes', None)]:\n",
    "                if label['attributes'].get('trafficLightColor', None):\n",
    "                    if label['attributes']['trafficLightColor'][1].lower() == 'green' or label['attributes']['trafficLightColor'][1].lower() == 'g'  or label['attributes']['trafficLightColor'][0] == 1:\n",
    "                        label['category'] = 'traffic light-green'\n",
    "                    elif label['attributes']['trafficLightColor'][1].lower() == 'yellow' or label['attributes']['trafficLightColor'][1].lower() == 'y'  or label['attributes']['trafficLightColor'][0] == 2:\n",
    "                        label['category'] = 'traffic light-amber'\n",
    "                    elif label['attributes']['trafficLightColor'][1].lower() == 'red' or label['attributes']['trafficLightColor'][1].lower() == 'r'  or label['attributes']['trafficLightColor'][0] == 3:\n",
    "                        label['category'] = 'traffic light-red'\n",
    "                    else:\n",
    "                        label['category'] = 'traffic light'\n",
    "\n",
    "                # Support both old and new bdd formats\n",
    "                elif label['attributes'].get('Traffic Light Color', None):\n",
    "                    if label['attributes']['Traffic Light Color'][1].lower() == 'g' or label['attributes']['Traffic Light Color'][0] == 1:\n",
    "                        label['category'] = 'traffic light-green'\n",
    "                    elif label['attributes']['Traffic Light Color'][1].lower() == 'y' or label['attributes']['Traffic Light Color'][0] == 2:\n",
    "                        label['category'] = 'traffic light-amber'\n",
    "                    elif label['attributes']['Traffic Light Color'][1].lower() == 'r' or label['attributes']['Traffic Light Color'][0] == 3:\n",
    "                        label['category'] = 'traffic light-red'\n",
    "                    else:\n",
    "                        label['category'] = 'traffic light'\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_categories = [ \"lane\", \"drivable area\"]\n",
    "\n",
    "def get_dataset(kache_db_api_url, dataset_names, save_dir):\n",
    "    base_url = kache_db_api_url + 'export/next_page'\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(\n",
    "        kache_db_auth_token)}\n",
    "    \n",
    "    datastream_finished = False\n",
    "    split_idx = 0\n",
    "    total_len = 0\n",
    "    batch = 0\n",
    "    dataset_dir = os.path.join(save_dir, '{}'.format('^'.join(dataset_names)))\n",
    "    os.makedirs(dataset_dir, exist_ok = True)\n",
    "    \n",
    "    while not datastream_finished:\n",
    "        body = {\"dataset_names\": dataset_names,\n",
    "               \"page_size\": 50000,\n",
    "               \"id\": split_idx}\n",
    "\n",
    "        try:\n",
    "            resp = requests.get(base_url, params=body, headers=headers)\n",
    "        except requests.exceptions.Timeout as e1:\n",
    "            # Maybe set up for a retry, or continue in a retry loop\n",
    "            print(e1)\n",
    "        except requests.exceptions.TooManyRedirects as e2:\n",
    "            # Tell the user their URL was bad and try a different one\n",
    "            print(e2)\n",
    "        except requests.exceptions.RequestException as e3:\n",
    "            # catastrophic error. Bail.\n",
    "            print(e3)\n",
    "            sys.exit(1)\n",
    "\n",
    "        if resp.status_code != 200:\n",
    "            raise requests.exceptions.RequestException(\n",
    "                f'GET /create status code: {resp.status_code}, content {resp.content}')\n",
    "\n",
    "        content_data = json.loads(resp.content)\n",
    "        content_data['data'] = update_traffic_light_cats(content_data['data'])\n",
    "        \n",
    "        with open(os.path.join(dataset_dir,'anns_{}.json'.format(batch)), 'w+') as fn:\n",
    "            json.dump(content_data['data'], fn)\n",
    "\n",
    "        # Check if query pulled back any data\n",
    "        if 'data' in content_data and len(content_data['data']) > 0:\n",
    "            with open(os.path.join(dataset_dir,'anns_{}.json'.format(batch)), 'w+') as fn:\n",
    "                json.dump(content_data['data'], fn)\n",
    "            frame_idxs = []\n",
    "            for frame in content_data['data']:\n",
    "                if 'labels' in frame:\n",
    "                    for lbl in frame['labels']:\n",
    "                        if 'category' in lbl and lbl['category'] not in excluded_categories:\n",
    "                            continue\n",
    "\n",
    "                else:\n",
    "                    print(f'No labels in the frame: {frame}')\n",
    "                frame_idxs.append(frame['kache_id'])\n",
    "            split_idx = frame_idxs[-1]\n",
    "            total_len += len(content_data[\"data\"])\n",
    "            print(f'{dataset_names} LENGTH ACCUMULATED: {total_len}')\n",
    "            print()\n",
    "        else:\n",
    "            print(f'No more data found in content. Last known split idx: {split_idx}')\n",
    "            datastream_finished = True\n",
    "            \n",
    "        batch +=1\n",
    "            \n",
    "    return dataset_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KACHE DB - AUTH_TOKEN: eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJleHAiOjE1NjA4NDM2MjIsImlhdCI6MTU1ODI1MTYyMiwic3ViIjoyfQ.YKDUM99F201uWXk9F10HrD58znGPh4-iEPs5g2bHBWE\n",
      "KACHE DB - AUTH_TOKEN_EXPIRY: 2019-06-18 00:40:22\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "# Create Model Subdirectory\n",
    "master_models_dir = '/media/dean/datastore/perception-pipelines/pipelines/models/'\n",
    "current_models_dir = os.path.join(master_models_dir,'1557540717')\n",
    "current_weights = weights_config['weights_path']\n",
    "cfg = 'configs/qc_cfg.tml'\n",
    "\n",
    "with open(cfg, 'r') as tmlfile:\n",
    "    cfg_data = toml.load(tmlfile)\n",
    "    csv_logger = cfg_data['testing']['csv_logger']\n",
    "    best_models_recorder = cfg_data['testing']['best_models_recorder']\n",
    "    # Get the Best Weights\n",
    "    if os.path.exists(best_models_recorder):\n",
    "        pkl_in = open(best_models_recorder, \"rb\")\n",
    "        best_models_cache = pickle.load(pkl_in)\n",
    "    else:\n",
    "        best_models_cache = {}\n",
    "\n",
    "\n",
    "    \n",
    "    cfg_priority_map = OrderedDict(dict(sorted(\n",
    "        cfg_data['priority_map'].items(), key=lambda t: t[1], reverse=True)))\n",
    "    \n",
    "    kache_db_api_url = cfg_data['kache_db']['api_url']\n",
    "\n",
    "    if make_tuple(cfg_data['kache_db']['username'])[0] in os.environ:\n",
    "        kache_db_username = make_tuple(\n",
    "            cfg_data['kache_db']['username'])[0]\n",
    "    else:\n",
    "        kache_db_username = make_tuple(\n",
    "            cfg_data['kache_db']['username'])[1]\n",
    "\n",
    "    if make_tuple(cfg_data['kache_db']['password'])[0] in os.environ:\n",
    "        kache_db_password = make_tuple(\n",
    "            cfg_data['kache_db']['password'])[0]\n",
    "    else:\n",
    "        kache_db_password = make_tuple(\n",
    "            cfg_data['kache_db']['password'])[1]\n",
    "\n",
    "\n",
    "if make_tuple(cfg_data['kache_db']['auth_token'])[0] in os.environ:\n",
    "        kache_db_auth_token = make_tuple(\n",
    "            cfg_data['kache_db']['auth_token'])[0]\n",
    "elif cfg_data['kache_db']['auth_token_expiry'] == \"\":\n",
    "    # Get new auth_token\n",
    "    kache_db_auth_token = get_kachedb_auth_token(kache_db_username, kache_db_password)\n",
    "\n",
    "    cur_time = datetime.datetime.strptime(\n",
    "        time.ctime(), \"%a %b %d %H:%M:%S %Y\")\n",
    "    kache_db_auth_token_expiry = cur_time + timedelta(days=30)\n",
    "\n",
    "    # Store expiry_date in config\n",
    "    cfg_data['kache_db']['auth_token_expiry'] = kache_db_auth_token_expiry\n",
    "    with open(cfg, 'w+') as f:\n",
    "        toml.dump(cfg_data, f)\n",
    "elif datetime.datetime.now() > datetime.datetime.strptime(str(cfg_data['kache_db']['auth_token_expiry']), \"%Y-%m-%d %H:%M:%S\"):\n",
    "    print(datetime.datetime.strptime(\n",
    "        str(cfg_data['kache_db']['auth_token_expiry']), \"%Y-%m-%d %H:%M:%S\"))\n",
    "    # Get new auth_token\n",
    "    print(kache_db_username,kache_db_password)\n",
    "    kache_db_auth_token = get_kachedb_auth_token(kache_db_username, kache_db_password)\n",
    "    cfg_data['kache_db']['auth_token'] = \"('KACHEDB_TOKEN','{}')\".format(\n",
    "        kache_db_auth_token)\n",
    "\n",
    "    cur_time = datetime.datetime.strptime(\n",
    "        time.ctime(), \"%a %b %d %H:%M:%S %Y\")\n",
    "    kache_db_auth_token_expiry = cur_time + timedelta(days=30)\n",
    "\n",
    "    # Store expiry_date in config\n",
    "    cfg_data['kache_db']['auth_token_expiry'] = kache_db_auth_token_expiry\n",
    "    with open(cfg, 'w+') as f:\n",
    "        toml.dump(cfg_data, f)\n",
    "else:\n",
    "    kache_db_auth_token_expiry = datetime.datetime.strptime(\n",
    "        str(cfg_data['kache_db']['auth_token_expiry']), \"%Y-%m-%d %H:%M:%S\")\n",
    "    kache_db_auth_token = make_tuple(\n",
    "        cfg_data['kache_db']['auth_token'])[1]\n",
    "\n",
    "\n",
    "\n",
    "print(\"KACHE DB - AUTH_TOKEN:\", kache_db_auth_token)\n",
    "print(\"KACHE DB - AUTH_TOKEN_EXPIRY:\", kache_db_auth_token_expiry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import http\n",
    "from functools import wraps\n",
    "import ntpath\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "\n",
    "\n",
    "def path_leaf(path):\n",
    "    if urllib.parse.urlparse(path).scheme != \"\" or os.path.isabs(path):\n",
    "        path = os.path.split(path)[-1]\n",
    "\n",
    "    head, tail = ntpath.split(path)\n",
    "    return tail or ntpath.basename(head)\n",
    "\n",
    "def retry(ExceptionToCheck, tries=4, delay=3, backoff=2, logger=None):\n",
    "        def deco_retry(f):\n",
    "            @wraps(f)\n",
    "            def f_retry(*args, **kwargs):\n",
    "                mtries, mdelay = tries, delay\n",
    "                while mtries > 1:\n",
    "                    try:\n",
    "                        return f(*args, **kwargs)\n",
    "                    except ExceptionToCheck as e:\n",
    "                        msg = \"%s, Retrying in %d seconds...\" % (str(e), mdelay)\n",
    "                        if logger:\n",
    "                            logger.warning(msg)\n",
    "                        else:\n",
    "                            print(msg)\n",
    "                        time.sleep(mdelay)\n",
    "                        mtries -= 1\n",
    "                        mdelay *= backoff\n",
    "                return f(*args, **kwargs)\n",
    "\n",
    "            return f_retry  # true decorator\n",
    "        return deco_retry\n",
    "    \n",
    "\n",
    "@retry(http.client.RemoteDisconnected, tries=5, delay=3, backoff=2)\n",
    "@retry(urllib.error.HTTPError, tries=5, delay=3, backoff=2)\n",
    "def urlrequest_with_retry(source, destination):\n",
    "    return urllib.request.urlretrieve(source, destination)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(dataset, image_dir):\n",
    "    count = 0\n",
    "    for idx, img_data in enumerate(dataset):\n",
    "        img_name = path_leaf(img_data['name'])        \n",
    "        destination = os.path.join(image_dir, img_name)\n",
    "        count +=1\n",
    "        if not os.path.exists(destination):\n",
    "            # Download image into dir\n",
    "            destination, _ = urllib.request.urlretrieve(img_data['url'], destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def format_groundtruths(dataset, gt_dir, weights_config, exclude_low_visibility = False):\n",
    "    for idx, img_data in enumerate(dataset):\n",
    "        img_name = os.path.splitext(path_leaf(img_data['name']))[0]      \n",
    "        destination = os.path.join(gt_dir, img_name+'.txt')\n",
    "        if not os.path.exists(destination):\n",
    "            # Generate Groundtruth data and save into file\n",
    "            all_anns = []\n",
    "            for ann in img_data['labels']:\n",
    "                \n",
    "                \n",
    "                # Filter holdoutset with only labels that match the testing config\n",
    "                if ann.get('box2d',None) and ann['category'] in weights_config['labels']:\n",
    "                    # Filter for low_visibility if flag true\n",
    "                    low_visibility = ann['attributes'].get('Low_Visibility',None)\n",
    "                    if not low_visibility: low_visibility = ann['attributes'].get('low_visibility',None)\n",
    "                    det_area = np.abs((ann['box2d']['x2'])-(ann['box2d']['x1']) * (ann['box2d']['y2'])-(ann['box2d']['y1']))                \n",
    "                    if exclude_low_visibility and low_visibility and (det_area < 10000):\n",
    "                        continue\n",
    "                    new_line = f\"{ann['category']} {int(ann['box2d']['x1'])} {int(ann['box2d']['y1'])} {int(ann['box2d']['x2'])} {int(ann['box2d']['y2'])}\"\n",
    "                    all_anns.append(new_line)\n",
    "                elif ann.get('box2d',None) and ann['category'].split('-')[0] in weights_config['labels']:\n",
    "                    new_line = f\"{ann['category'].split('-')[0]} {int(ann['box2d']['x1'])} {int(ann['box2d']['y1'])} {int(ann['box2d']['x2'])} {int(ann['box2d']['y2'])}\"\n",
    "                    all_anns.append(new_line)\n",
    "                    \n",
    "            # Write labels to file\n",
    "            with open(destination, 'w') as f:\n",
    "                for item in all_anns:\n",
    "                    f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "def format_detections(image_dir, dets_dir):    \n",
    "    yolo_dir = \"/media/dean/datastore/perception-pipelines/pipelines/nets/ObjectDetection-OneStageDet/yolo/\"    \n",
    "    yolo_cfg = os.path.join(yolo_dir,'cfgs/yolov3.yml')\n",
    "    # Load yaml and replace the anchors/labels\n",
    "    with open(yolo_cfg, 'r') as ymlfile:\n",
    "        cfg = yaml.load(ymlfile)\n",
    "\n",
    "        cfg['labels'] = weights_config['labels']\n",
    "        cfg['anchors'] = weights_config['anchors']\n",
    "        \n",
    "    with open(yolo_cfg, 'w') as outfile:\n",
    "        yaml.dump(cfg, outfile, default_flow_style=False)\n",
    "    \n",
    "    anns_dir = os.path.join(os.path.abspath(image_dir), 'annotations')\n",
    "    os.makedirs(anns_dir, exist_ok = True)\n",
    "\n",
    "    cmd = \"cd {} && python3 pipelines/annotate.py -c -weight {} -image_dir {} -output_dir {}\".format(yolo_dir, weights_config['weights_path'], os.path.abspath(image_dir), anns_dir)\n",
    "    res = os.system(cmd)\n",
    "    anns_path = os.path.join(anns_dir, 'annotations.json')\n",
    "\n",
    "    with open(anns_path, 'r') as f:\n",
    "        anns = json.load(f) \n",
    "\n",
    "        for uri, img_anns in anns:\n",
    "            uri = os.path.splitext(path_leaf(uri))[0]\n",
    "            destination = os.path.join(dets_dir, uri+'.txt')\n",
    "            if not os.path.exists(destination):\n",
    "                # Generate Groundtruth data and save into file\n",
    "                all_anns = []\n",
    "                for ann in img_anns:\n",
    "                    if ann.get('box2d',None):\n",
    "                        new_line = f\"{ann['category']} {ann['confidence']} {int(ann['box2d']['x1'])} {int(ann['box2d']['y1'])} {int(ann['box2d']['x2'])} {int(ann['box2d']['y2'])}\"\n",
    "                        all_anns.append(new_line)\n",
    "\n",
    "                # Write labels to file\n",
    "                with open(destination, 'w') as f:\n",
    "                    for item in all_anns:\n",
    "                        f.write(\"%s\\n\" % item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the steps below to start evaluating your detections:\n",
    "\n",
    "1. [Create the ground truth files](#create-the-ground-truth-files)\n",
    "2. [Create your detection files](#create-your-detection-files)\n",
    "3. For **Pascal VOC metrics**, run the command: `python pascalvoc.py`  \n",
    "   If you want to reproduce the example above, run the command: `python pascalvoc.py -t 0.3`\n",
    "4. (Optional) [You can use arguments to control the IOU threshold, bounding boxes format, etc.](#optional-arguments)\n",
    "\n",
    "### Create the ground truth files\n",
    "\n",
    "- Create a separate ground truth text file for each image in the folder **groundtruths/**.\n",
    "- In these files each line should be in the format: `<class_name> <left> <top> <right> <bottom>`.    \n",
    "- E.g. The ground truth bounding boxes of the image \"2008_000034.jpg\" are represented in the file \"2008_000034.txt\":\n",
    "  ```\n",
    "  bottle 6 234 45 362\n",
    "  person 1 156 103 336\n",
    "  person 36 111 198 416\n",
    "  person 91 42 338 500\n",
    "  ```\n",
    "    \n",
    "If you prefer, you can also have your bounding boxes in the format: `<class_name> <left> <top> <width> <height>` (see here [**\\***](#asterisk) how to use it). In this case, your \"2008_000034.txt\" would be represented as:\n",
    "  ```\n",
    "  bottle 6 234 39 128\n",
    "  person 1 156 102 180\n",
    "  person 36 111 162 305\n",
    "  person 91 42 247 458\n",
    "  ```\n",
    "\n",
    "### Create your detection files\n",
    "\n",
    "- Create a separate detection text file for each image in the folder **detections/**.\n",
    "- The names of the detection files must match their correspond ground truth (e.g. \"detections/2008_000182.txt\" represents the detections of the ground truth: \"groundtruths/2008_000182.txt\").\n",
    "- In these files each line should be in the following format: `<class_name> <confidence> <left> <top> <right> <bottom>` (see here [**\\***](#asterisk) how to use it).\n",
    "- E.g. \"2008_000034.txt\":\n",
    "    ```\n",
    "    bottle 0.14981 80 1 295 500  \n",
    "    bus 0.12601 36 13 404 316  \n",
    "    horse 0.12526 430 117 500 307  \n",
    "    pottedplant 0.14585 212 78 292 118  \n",
    "    tvmonitor 0.070565 388 89 500 196  \n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/26 [00:00<?, ?batch/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 1/26 [00:00<00:24,  1.01batch/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 2/26 [00:01<00:21,  1.10batch/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▌        | 4/26 [00:02<00:16,  1.33batch/s]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▉        | 5/26 [00:03<00:17,  1.21batch/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 6/26 [00:04<00:15,  1.25batch/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 7/26 [00:05<00:15,  1.22batch/s]\u001b[A\u001b[A\n",
      "\n",
      " 31%|███       | 8/26 [00:06<00:15,  1.14batch/s]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███▍      | 9/26 [00:06<00:14,  1.18batch/s]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 10/26 [00:07<00:13,  1.23batch/s]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 11/26 [00:08<00:12,  1.25batch/s]\u001b[A\u001b[A\n",
      "\n",
      " 46%|████▌     | 12/26 [00:08<00:10,  1.36batch/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 13/26 [00:09<00:09,  1.37batch/s]\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████▍    | 14/26 [00:10<00:08,  1.42batch/s]\u001b[A\u001b[A\n",
      "\n",
      " 58%|█████▊    | 15/26 [00:11<00:07,  1.41batch/s]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▏   | 16/26 [00:11<00:06,  1.47batch/s]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████▌   | 17/26 [00:12<00:06,  1.47batch/s]\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████▉   | 18/26 [00:13<00:05,  1.47batch/s]\u001b[A\u001b[A\n",
      "\n",
      " 73%|███████▎  | 19/26 [00:13<00:04,  1.43batch/s]\u001b[A\u001b[A\n",
      "\n",
      " 77%|███████▋  | 20/26 [00:14<00:04,  1.45batch/s]\u001b[A\u001b[A\n",
      "\n",
      " 81%|████████  | 21/26 [00:15<00:03,  1.44batch/s]\u001b[A\u001b[A\n",
      "\n",
      " 85%|████████▍ | 22/26 [00:15<00:02,  1.42batch/s]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 23/26 [00:16<00:02,  1.42batch/s]\u001b[A\u001b[A\n",
      "\n",
      " 92%|█████████▏| 24/26 [00:17<00:01,  1.36batch/s]\u001b[A\u001b[A\n",
      "\n",
      " 96%|█████████▌| 25/26 [00:18<00:00,  1.41batch/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 26/26 [00:18<00:00,  1.43batch/s]\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# Get Holdout set\n",
    "cache_dir = '/media/dean/datastore/perception-pipelines/pipelines/kache'\n",
    "#holdout_set_dir = get_dataset(kache_db_api_url, ['holdout_set','kache_set'], cache_dir)\n",
    "holdout_set_dir = '/media/dean/datastore/perception-pipelines/pipelines/kache/holdout_set^kache_set/'\n",
    "\n",
    "# Get Images and store in .kache/holdout_set/images\n",
    "img_dir = os.path.join(holdout_set_dir, 'images')\n",
    "gt_dir =  os.path.join(holdout_set_dir, 'groundtruths')\n",
    "easymode_gt_dir = os.path.join(holdout_set_dir, 'easy-groundtruths')\n",
    "dets_dir = os.path.join(holdout_set_dir, 'detections')\n",
    "\n",
    "os.makedirs(img_dir, exist_ok = True)\n",
    "os.makedirs(gt_dir, exist_ok = True)\n",
    "os.makedirs(easymode_gt_dir, exist_ok = True)\n",
    "os.makedirs(dets_dir, exist_ok = True)\n",
    "\n",
    "holdout_set_batches = glob.glob(os.path.join(holdout_set_dir, '*.json'))\n",
    "for holdout_set_batch in tqdm(holdout_set_batches, unit='batch'):\n",
    "    with open(holdout_set_batch, 'r') as f:\n",
    "        holdout_set = json.load(f)\n",
    "        # If s3 bucket returns data, store it locally\n",
    "        ##TODO: Download dataset cache from s3\n",
    "        # After download dataset cache from s3, verify each frame lives in directory\n",
    "        get_images(holdout_set, img_dir)\n",
    "\n",
    "\n",
    "        # Convert Ground Truth into Object Detection Metrics format in Model Subdirectory if not exist\n",
    "        format_groundtruths(holdout_set, gt_dir, weights_config)\n",
    "        # Easy Mode\n",
    "        format_groundtruths(holdout_set, easymode_gt_dir, weights_config, exclude_low_visibility = True)\n",
    "        \n",
    "# Convert Anns into Object Detection Metrics format in Model Subdirectory if not exist\n",
    "format_detections(img_dir, dets_dir)\n",
    "\n",
    "\n",
    "## Read in SCS Message from \"Testing Pipeline\" Queue ##\n",
    "# Get weights path (On Georgetown) from SCS Message\n",
    "# Get weights config from SCS Message\n",
    "\n",
    "\n",
    "\n",
    "# Run inference with message weights against Holdoutset\n",
    "\n",
    "\n",
    "## Run testing suite ##\n",
    "test_metrics_dir = '/media/dean/datastore/perception-pipelines/pipelines/utils/Object-Detection-Metrics'\n",
    "results_dir = os.path.join(holdout_set_dir,'testing_results/')\n",
    "os.makedirs(results_dir, exist_ok = True)\n",
    "cmd = \"cd {} && python pascalvoc.py -det {} -gt {} -sp {} | tee -a {}\".format(test_metrics_dir, os.path.abspath(dets_dir), os.path.abspath(gt_dir), os.path.abspath(results_dir), os.path.join(results_dir, 'testings_results.txt'))\n",
    "res = os.system(cmd)\n",
    "\n",
    "## Run testing suite in 'easy mode' without low_visibility detections in ground truth ## \n",
    "easymode_results_dir =os.path.join(holdout_set_dir,'easymode_testing_results/')\n",
    "os.makedirs(easymode_results_dir, exist_ok = True)\n",
    "cmd = \"cd {} && python pascalvoc.py -det {} -gt {} -sp {} | tee -a {}\".format(test_metrics_dir, os.path.abspath(dets_dir), os.path.abspath(easymode_gt_dir), os.path.abspath(easymode_results_dir), os.path.join(easymode_results_dir, 'easymode_testings_results.txt'))\n",
    "res = os.system(cmd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mAP metrics on Model Path: /media/dean/datastore/perception-pipelines/pipelines/models/1557540719/weights/weights_1000.pt\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/media/dean/datastore/perception-pipelines/pipelines/kache/holdout_set^kache_set/testing_results/results.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-da4a47b1c1fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m#Get mAP results from file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mclass_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_stats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'map_results_file'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopenfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/media/dean/datastore/perception-pipelines/pipelines/kache/holdout_set^kache_set/testing_results/results.txt'"
     ]
    }
   ],
   "source": [
    "# Collect Metrics from results file \n",
    "print('Collecting mAP metrics on Model Path:', weights_config['weights_path'])\n",
    "testing_results = [results_dir, easymode_results_dir]\n",
    "results = []\n",
    "\n",
    "for test in testing_results:\n",
    "    \n",
    "    # Grab iterations and sort checkpoints\n",
    "    fname = path_leaf(weights_config['weights_path'])\n",
    "    iterations = os.path.splitext(fname)[0].replace('weights_','')\n",
    "\n",
    "    map_stats = {}\n",
    "    map_stats['trn_iterations'] = iterations\n",
    "    map_stats['map_results_file'] = os.path.join(str(test), 'results.txt')\n",
    "    if 'easy' in test:\n",
    "        map_stats['testing_mode'] = 'easymode'\n",
    "    else:\n",
    "        map_stats['testing_mode'] = 'normal'   \n",
    "\n",
    "\n",
    "    #Get mAP results from file\n",
    "    class_stats = []\n",
    "    with open(map_stats['map_results_file']) as openfile:\n",
    "        lines = openfile.readlines()\n",
    "        for i, line in enumerate(lines): \n",
    "            if 'Class:' in line:\n",
    "                class_map = {} \n",
    "                class_map['class_name'] = line.split('Class:')[1].strip('\\n').strip()\n",
    "                next_line = lines[i+1]\n",
    "                # Get class AP\n",
    "                if 'AP:' in next_line:\n",
    "                    class_map['class_ap'] = next_line.split('AP:')[1].strip('\\n').strip()\n",
    "                class_stats.append(class_map)\n",
    "            elif 'mAP:' in line:\n",
    "                map_stats['mean_avg_precision'] = line.split('mAP:')[1].strip('\\n').strip()\n",
    "\n",
    "    map_stats['class_stats'] = class_stats\n",
    "    results.append(map_stats)\n",
    "\n",
    "    # Cache Data    \n",
    "    pickle_file = os.path.join(results_dir, 'mAP_{}.pickle'.format(map_stats['testing_mode']))\n",
    "    pickle_dict = {'map_stats': map_stats}\n",
    "    with open(pickle_file,\"wb\") as pickle_out:\n",
    "        pickle.dump(pickle_dict, pickle_out)\n",
    "\n",
    "print(results,'\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "class CSVLogger:\n",
    "    def __init__(self, path, filename, fields):\n",
    "\n",
    "        # Create csv file\n",
    "        file_path = os.path.join(path, filename)\n",
    "        \n",
    "\n",
    "        # Initialize writer\n",
    "        if not os.path.exists(file_path):\n",
    "            os.makedirs(path, exist_ok = True)\n",
    "            self.file = open(file_path, 'a')\n",
    "            self.csv_writer = csv.DictWriter(self.file, fieldnames=fields, delimiter=',')\n",
    "            self.csv_writer.writeheader()\n",
    "        else:\n",
    "            self.file = open(file_path, 'a')\n",
    "            self.csv_writer = csv.DictWriter(self.file, fieldnames=fields, delimiter=',')\n",
    "\n",
    "    def record(self, values_dict):\n",
    "        self.csv_writer.writerow(values_dict)\n",
    "        \n",
    "    def close(self):\n",
    "        self.file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "\n",
    "## Format Testing Results into dictionary ##\n",
    "\n",
    "filtered_labels = sorted(weights_config['labels'])\n",
    "# Filter based on model labels\n",
    "filtered_scores = OrderedDict()\n",
    "# Get priority map from cfg\n",
    "weights = list(cfg_priority_map.values())\n",
    "# Normalize priority map\n",
    "x = np.array(weights)\n",
    "weights_norm = x / np.linalg.norm(x)\n",
    "weights_norm = weights_norm / np.sum(weights_norm)\n",
    "\n",
    "sorted_weightlist = OrderedDict()\n",
    "for label in filtered_labels:\n",
    "    sorted_weightlist[label.replace('-','').replace('_','').replace(' ','').lower()] = weights_norm[list(cfg_priority_map.keys()).index(label)]\n",
    "\n",
    "\n",
    "model_cfg_key = '|'.join(filtered_labels).replace(' ', '').replace('-', '')\n",
    "\n",
    "test_data = []\n",
    "for map_stats in results:\n",
    "    d = {}\n",
    "    d['job_id'] = weights_config['job_id']\n",
    "    d['testing_mode'] = map_stats['testing_mode']\n",
    "    d['map_results_file'] = map_stats['map_results_file']\n",
    "    d['iterations'] = map_stats['trn_iterations']\n",
    "    if map_stats.get('mean_avg_precision', None):\n",
    "        d['mean_avg_precision'] = map_stats['mean_avg_precision']\n",
    "    else:d['mean_avg_precision'] = 0.0\n",
    "    \n",
    "    # Initialize mAP values\n",
    "    d['person_ap'], d['rider_ap'], d['car_ap'], d['truck_ap'], d['bus_ap'], d['train_ap'],\\\n",
    "    d['motorcycle_ap'], d['bike_ap'], d['traffic_sign_ap'], d['traffic_light_ap'], \\\n",
    "    d['trailer_ap'], d['construct-cone_ap'], d['construct-sign_ap'], d['construct-barrel_ap'], \\\n",
    "    d['construct-pole_ap'],d['construct-post_ap'], d['construct-equipment_ap'], d['traffic_light-red_ap'], d['traffic_light-amber_ap'], \\\n",
    "    d['traffic_light-green_ap'], d['traffic_sign-stop_sign_ap'], d['traffic_sign-slow_sign_ap'], d['traffic_sign-speed_sign_ap']= [0.0]*23\n",
    "\n",
    "\n",
    "    for cls in map_stats['class_stats']:\n",
    "        cls_name = cls.get('class_name', '').strip()\n",
    "        if cls_name == 'person':\n",
    "            d['person_ap'] = cls['class_ap']\n",
    "        elif cls_name == 'rider':\n",
    "            d['rider_ap'] = cls['class_ap']\n",
    "        elif cls_name == 'car':\n",
    "            d['car_ap'] = cls['class_ap']\n",
    "        elif cls_name == 'truck':\n",
    "            d['truck_ap'] = cls['class_ap']\n",
    "        elif cls_name == 'bus':\n",
    "            d['bus_ap'] = cls['class_ap']\n",
    "        elif cls_name == 'train':\n",
    "            d['train_ap'] = cls['class_ap']\n",
    "        elif cls_name == 'motor':\n",
    "            d['motor_ap'] = cls['class_ap']\n",
    "        elif cls_name == 'bike':\n",
    "            d['bike_ap'] = cls['class_ap']\n",
    "        elif cls_name == 'traffic-sign':\n",
    "            d['traffic_sign_ap'] = cls['class_ap']\n",
    "        elif cls_name == 'traffic-light':\n",
    "            d['traffic_light_ap'] = cls['class_ap']\n",
    "        elif cls_name == 'traffic-light-green':\n",
    "            d['traffic_light_green_ap'] = cls['class_ap']\n",
    "        elif cls_name == 'traffic-light-red':\n",
    "            d['traffic_light_red_ap'] = cls['class_ap']\n",
    "        elif cls_name == 'traffic-light-amber':\n",
    "            d['traffic_light_amber_ap'] = cls['class_ap']\n",
    "        elif cls_name == 'trailer':\n",
    "            d['trailer_ap'] = cls['class_ap']\n",
    "        elif cls_name == 'construct-cone':\n",
    "            d['construct-cone_ap'] = cls['class_ap']\n",
    "        elif cls_name == 'construct-sign':\n",
    "            d['construct-sign_ap'] = cls['class_ap']\n",
    "        elif cls_name == 'construct-barrel':\n",
    "            d['construct-barrel_ap'] = cls['class_ap']\n",
    "        elif cls_name == 'construct-pole':\n",
    "            d['construct-pole_ap'] = cls['class_ap']\n",
    "        elif cls_name == 'construct-post':\n",
    "            d['construct-post_ap'] = cls['class_ap']\n",
    "        elif cls_name == 'construct-equipment':\n",
    "            d['construct-equipment_ap'] = cls['class_ap']\n",
    "        elif cls_name == 'traffic-sign-stop_sign':\n",
    "            d['traffic_sign-stop_sign_ap'] = cls['class_ap']\n",
    "        elif cls_name == 'traffic-sign-slow_sign':\n",
    "            d['traffic_sign-stop_slow_ap'] = cls['class_ap']\n",
    "        elif cls_name == 'traffic-sign-speed_sign':\n",
    "            d['traffic_sign-speed_sign_ap'] = cls['class_ap']\n",
    "            \n",
    "        \n",
    "            \n",
    "        if cls_name.replace('-','').replace('_','').replace(' ','').lower() in [x.replace('-','').replace('_','').replace(' ','').lower() for x in filtered_labels] and map_stats['testing_mode'] == 'normal':\n",
    "            filtered_scores[cls_name] = float(cls['class_ap'].strip('%'))*sorted_weightlist[cls_name.replace('-','').replace('_','').replace(' ','').lower()] # 54.50% * .75\n",
    "            filtered_scores['mAP'] = float(d['mean_avg_precision'].strip('%'))* \\\n",
    "                weights_norm[list(cfg_priority_map.keys()).index('mean_avg_precision')]\n",
    "                \n",
    "        elif 'easymode' in map_stats['testing_mode']:\n",
    "            filtered_scores['easy_mAP'] = float(d['mean_avg_precision'].strip('%'))* \\\n",
    "                weights_norm[list(cfg_priority_map.keys()).index('easymode_mean_avg_precision')]\n",
    "    test_data.append(d)\n",
    "\n",
    "# Calculate Test Score and Log Result\n",
    "current_model_test_score = np.sum(list(filtered_scores.values()))\n",
    "TESTSCORE_FIELDS = list(test_data[0].keys())\n",
    "TESTSCORE_FIELDS.append('test_score')\n",
    "csv_frames_logger = CSVLogger(os.path.dirname(csv_logger), path_leaf(csv_logger), TESTSCORE_FIELDS)\n",
    "for data in test_data:\n",
    "    data['test_score'] = current_model_test_score\n",
    "    csv_frames_logger.record(data)\n",
    "csv_frames_logger.close()\n",
    "\n",
    "data = pd.DataFrame(test_data)\n",
    "data = data.apply(pd.to_numeric, errors='ignore')\n",
    "data.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_models_cache.get(model_cfg_key, None):\n",
    "    best_model_test_score = 10000\n",
    "    if current_model_test_score > best_model_test_score:       \n",
    "        # Copy weights into new directory (directory_name -> timestamp) under best_models_dir\n",
    "        \n",
    "        # Update cache\n",
    "        print('NEW BEST MODEL')\n",
    "else:\n",
    "    # Assign current weights as 'best_model', copy\n",
    "    # Copy weights into new directory (directory_name -> timestamp) under best_models_dir\n",
    "    best_models_dir = os.path.dirname(best_models_recorder)\n",
    "    d = datetime.datetime.utcnow()\n",
    "    epoch = datetime.datetime(1970,1,1)\n",
    "    best_model_timestamp = int((d - epoch).total_seconds())\n",
    "    best_model_weights_path = os.path.join(best_models_dir, str(best_model_timestamp), path_leaf(weights_config['weights_path']))\n",
    "    os.makedirs(os.path.dirname(best_model_weights_path), exist_ok = True)\n",
    "    shutil.copy(weights_config['weights_path'], best_model_weights_path)\n",
    "    \n",
    "    # Save Labels and Anchors as json\n",
    "    model_cfg = os.path.join(os.path.dirname(best_model_weights_path), 'cfg.json')\n",
    "    with open(model_cfg, 'w+') as fn:\n",
    "        weights_config['weights_path'] = best_model_weights_path\n",
    "        json.dump(weights_config, fn)\n",
    "    \n",
    "    best_models_cache[model_cfg_key] = best_model_weights_path\n",
    "    # Save cache\n",
    "    with open(best_models_recorder, 'wb') as fn:\n",
    "        pickle.dump(best_models_cache, fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_models_cache.get(model_cfg_key):\n",
    "    best_models_weights = best_models_cache[model_cfg_key]\n",
    "    weights_cfg_file = os.path.join(os.path.dirname(best_models_weights), 'cfg.json')\n",
    "    with open(weights_cfg_file, 'r') as f:\n",
    "        weights_config = json.load(f)\n",
    "        print(weights_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Get Holdout set\n",
    "cache_dir = '/media/dean/datastore/perception-pipelines/pipelines/kache'\n",
    "#holdout_set_dir = get_dataset(kache_db_api_url, ['china_set'], cache_dir)\n",
    "holdout_set_dir = '/media/dean/datastore/perception-pipelines/pipelines/kache/china_set/'\n",
    "\n",
    "# Get Images and store in .kache/holdout_set/images\n",
    "img_dir = os.path.join(holdout_set_dir, 'images')\n",
    "gt_dir =  os.path.join(holdout_set_dir, 'groundtruths')\n",
    "easymode_gt_dir = os.path.join(holdout_set_dir, 'easy-groundtruths')\n",
    "dets_dir = os.path.join(holdout_set_dir, 'bestmodel_detections')\n",
    "\n",
    "os.makedirs(img_dir, exist_ok = True)\n",
    "os.makedirs(gt_dir, exist_ok = True)\n",
    "os.makedirs(easymode_gt_dir, exist_ok = True)\n",
    "os.makedirs(dets_dir, exist_ok = True)\n",
    "\n",
    "holdout_set_batches = glob.glob(os.path.join(holdout_set_dir, '*.json'))\n",
    "for holdout_set_batch in tqdm(holdout_set_batches, unit='batch'):\n",
    "    with open(holdout_set_batch, 'r') as f:\n",
    "        holdout_set = json.load(f)\n",
    "        # If s3 bucket returns data, store it locally\n",
    "        ##TODO: Download dataset cache from s3\n",
    "        # After download dataset cache from s3, verify each frame lives in directory\n",
    "        get_images(holdout_set, img_dir)\n",
    "\n",
    "\n",
    "        # Convert Ground Truth into Object Detection Metrics format in Model Subdirectory if not exist\n",
    "        format_groundtruths(holdout_set, gt_dir, weights_config)\n",
    "        # Easy Mode\n",
    "        format_groundtruths(holdout_set, easymode_gt_dir, weights_config, exclude_low_visibility = True)\n",
    "        \n",
    "# Convert Anns into Object Detection Metrics format in Model Subdirectory if not exist\n",
    "format_detections(img_dir, dets_dir)\n",
    "\n",
    "\n",
    "## Read in SCS Message from \"Testing Pipeline\" Queue ##\n",
    "# Get weights path (On Georgetown) from SCS Message\n",
    "# Get weights config from SCS Message\n",
    "\n",
    "\n",
    "\n",
    "# Run inference with message weights against Holdoutset\n",
    "\n",
    "\n",
    "## Run testing suite ##\n",
    "test_metrics_dir = '/media/dean/datastore/perception-pipelines/pipelines/utils/Object-Detection-Metrics'\n",
    "results_dir = os.path.join(holdout_set_dir,'bestmodel_testing_results/')\n",
    "os.makedirs(results_dir, exist_ok = True)\n",
    "cmd = \"cd {} && python pascalvoc.py -det {} -gt {} -sp {} | tee -a {}\".format(test_metrics_dir, os.path.abspath(dets_dir), os.path.abspath(gt_dir), os.path.abspath(results_dir), os.path.join(results_dir, 'testings_results.txt'))\n",
    "res = os.system(cmd)\n",
    "\n",
    "## Run testing suite in 'easy mode' without low_visibility detections in ground truth ## \n",
    "easymode_results_dir =os.path.join(holdout_set_dir,'bestmodel_easymode_testing_results/')\n",
    "os.makedirs(easymode_results_dir, exist_ok = True)\n",
    "cmd = \"cd {} && python pascalvoc.py -det {} -gt {} -sp {} | tee -a {}\".format(test_metrics_dir, os.path.abspath(dets_dir), os.path.abspath(easymode_gt_dir), os.path.abspath(easymode_results_dir), os.path.join(easymode_results_dir, 'easymode_testings_results.txt'))\n",
    "res = os.system(cmd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect Metrics from results file \n",
    "print('Collecting mAP metrics on Model Path:', weights_config['weights_path'])\n",
    "testing_results = [results_dir, easymode_results_dir]\n",
    "results = []\n",
    "\n",
    "for test in testing_results:\n",
    "    \n",
    "    # Grab iterations and sort checkpoints\n",
    "    fname = path_leaf(weights_config['weights_path'])\n",
    "    iterations = os.path.splitext(fname)[0].replace('weights_','')\n",
    "\n",
    "    map_stats = {}\n",
    "    map_stats['trn_iterations'] = iterations\n",
    "    map_stats['map_results_file'] = os.path.join(str(test), 'results.txt')\n",
    "    if 'easy' in test:\n",
    "        map_stats['testing_mode'] = 'easymode'\n",
    "    else:\n",
    "        map_stats['testing_mode'] = 'normal'   \n",
    "\n",
    "\n",
    "    #Get mAP results from file\n",
    "    class_stats = []\n",
    "    with open(map_stats['map_results_file']) as openfile:\n",
    "        lines = openfile.readlines()\n",
    "        for i, line in enumerate(lines): \n",
    "            if 'Class:' in line:\n",
    "                class_map = {} \n",
    "                class_map['class_name'] = line.split('Class:')[1].strip('\\n').strip()\n",
    "                next_line = lines[i+1]\n",
    "                # Get class AP\n",
    "                if 'AP:' in next_line:\n",
    "                    class_map['class_ap'] = next_line.split('AP:')[1].strip('\\n').strip()\n",
    "                class_stats.append(class_map)\n",
    "            elif 'mAP:' in line:\n",
    "                map_stats['mean_avg_precision'] = line.split('mAP:')[1].strip('\\n').strip()\n",
    "\n",
    "    map_stats['class_stats'] = class_stats\n",
    "    results.append(map_stats)\n",
    "\n",
    "    # Cache Data    \n",
    "    pickle_file = os.path.join(results_dir, 'mAP_{}.pickle'.format(map_stats['testing_mode']))\n",
    "    pickle_dict = {'map_stats': map_stats}\n",
    "    with open(pickle_file,\"wb\") as pickle_out:\n",
    "        pickle.dump(pickle_dict, pickle_out)\n",
    "\n",
    "print(results,'\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "\n",
    "## Format Testing Results into dictionary ##\n",
    "\n",
    "filtered_labels = sorted(weights_config['labels'])\n",
    "# Filter based on model labels\n",
    "filtered_scores = OrderedDict()\n",
    "# Get priority map from cfg\n",
    "weights = list(cfg_priority_map.values())\n",
    "# Normalize priority map\n",
    "x = np.array(weights)\n",
    "weights_norm = x / np.linalg.norm(x)\n",
    "weights_norm = weights_norm / np.sum(weights_norm)\n",
    "\n",
    "sorted_weightlist = OrderedDict()\n",
    "for label in filtered_labels:\n",
    "    sorted_weightlist[label.replace('-','').replace('_','').replace(' ','').lower()] = weights_norm[list(cfg_priority_map.keys()).index(label)]\n",
    "\n",
    "\n",
    "model_cfg_key = '|'.join(filtered_labels).replace(' ', '').replace('-', '')\n",
    "\n",
    "test_data = []\n",
    "for map_stats in results:\n",
    "    d = {}\n",
    "    d['job_id'] = weights_config['job_id']\n",
    "    d['testing_mode'] = map_stats['testing_mode']\n",
    "    d['map_results_file'] = map_stats['map_results_file']\n",
    "    d['iterations'] = map_stats['trn_iterations']\n",
    "    if map_stats.get('mean_avg_precision', None):\n",
    "        d['mean_avg_precision'] = map_stats['mean_avg_precision']\n",
    "    else:d['mean_avg_precision'] = 0.0\n",
    "    \n",
    "    # Initialize mAP values\n",
    "    d['person_ap'], d['rider_ap'], d['car_ap'], d['truck_ap'], d['bus_ap'], d['train_ap'],\\\n",
    "    d['motorcycle_ap'], d['bike_ap'], d['traffic_sign_ap'], d['traffic_light_ap'], \\\n",
    "    d['trailer_ap'], d['construct-cone_ap'], d['construct-sign_ap'], d['construct-barrel_ap'], \\\n",
    "    d['construct-pole_ap'],d['construct-post_ap'], d['construct-equipment_ap'], d['traffic_light-red_ap'], d['traffic_light-amber_ap'], \\\n",
    "    d['traffic_light-green_ap'], d['traffic_sign-stop_sign_ap'], d['traffic_sign-slow_sign_ap'], d['traffic_sign-speed_sign_ap']= [0.0]*23\n",
    "\n",
    "\n",
    "    for cls in map_stats['class_stats']:\n",
    "        cls_name = cls.get('class_name', '').strip()\n",
    "        if cls_name == 'person':\n",
    "            d['person_ap'] = cls['class_ap']\n",
    "        elif cls_name == 'rider':\n",
    "            d['rider_ap'] = cls['class_ap']\n",
    "        elif cls_name == 'car':\n",
    "            d['car_ap'] = cls['class_ap']\n",
    "        elif cls_name == 'truck':\n",
    "            d['truck_ap'] = cls['class_ap']\n",
    "        elif cls_name == 'bus':\n",
    "            d['bus_ap'] = cls['class_ap']\n",
    "        elif cls_name == 'train':\n",
    "            d['train_ap'] = cls['class_ap']\n",
    "        elif cls_name == 'motor':\n",
    "            d['motor_ap'] = cls['class_ap']\n",
    "        elif cls_name == 'bike':\n",
    "            d['bike_ap'] = cls['class_ap']\n",
    "        elif cls_name == 'traffic-sign':\n",
    "            d['traffic_sign_ap'] = cls['class_ap']\n",
    "        elif cls_name == 'traffic-light':\n",
    "            d['traffic_light_ap'] = cls['class_ap']\n",
    "        elif cls_name == 'traffic-light-green':\n",
    "            d['traffic_light_green_ap'] = cls['class_ap']\n",
    "        elif cls_name == 'traffic-light-red':\n",
    "            d['traffic_light_red_ap'] = cls['class_ap']\n",
    "        elif cls_name == 'traffic-light-amber':\n",
    "            d['traffic_light_amber_ap'] = cls['class_ap']\n",
    "        elif cls_name == 'trailer':\n",
    "            d['trailer_ap'] = cls['class_ap']\n",
    "        elif cls_name == 'construct-cone':\n",
    "            d['construct-cone_ap'] = cls['class_ap']\n",
    "        elif cls_name == 'construct-sign':\n",
    "            d['construct-sign_ap'] = cls['class_ap']\n",
    "        elif cls_name == 'construct-barrel':\n",
    "            d['construct-barrel_ap'] = cls['class_ap']\n",
    "        elif cls_name == 'construct-pole':\n",
    "            d['construct-pole_ap'] = cls['class_ap']\n",
    "        elif cls_name == 'construct-post':\n",
    "            d['construct-post_ap'] = cls['class_ap']\n",
    "        elif cls_name == 'construct-equipment':\n",
    "            d['construct-equipment_ap'] = cls['class_ap']\n",
    "        elif cls_name == 'traffic-sign-stop_sign':\n",
    "            d['traffic_sign-stop_sign_ap'] = cls['class_ap']\n",
    "        elif cls_name == 'traffic-sign-slow_sign':\n",
    "            d['traffic_sign-stop_slow_ap'] = cls['class_ap']\n",
    "        elif cls_name == 'traffic-sign-speed_sign':\n",
    "            d['traffic_sign-speed_sign_ap'] = cls['class_ap']\n",
    "        \n",
    "                    \n",
    "        if cls_name.replace('-','').replace('_','').replace(' ','').lower() in [x.replace('-','').replace('_','').replace(' ','').lower() for x in filtered_labels] and map_stats['testing_mode'] == 'normal':\n",
    "            filtered_scores[cls_name] = float(cls['class_ap'].strip('%'))*sorted_weightlist[cls_name.replace('-','').replace('_','').replace(' ','').lower()] # 54.50% * .75\n",
    "            filtered_scores['mAP'] = float(d['mean_avg_precision'].strip('%'))* \\\n",
    "                weights_norm[list(cfg_priority_map.keys()).index('mean_avg_precision')]\n",
    "                \n",
    "        elif 'easymode' in map_stats['testing_mode']:\n",
    "            filtered_scores['easy_mAP'] = float(d['mean_avg_precision'].strip('%'))* \\\n",
    "                weights_norm[list(cfg_priority_map.keys()).index('easymode_mean_avg_precision')]\n",
    "            \n",
    "    test_data.append(d)\n",
    "    \n",
    "\n",
    "# Calculate Test Score and Log Result\n",
    "best_model_test_score = np.sum(list(filtered_scores.values()))\n",
    "TESTSCORE_FIELDS = list(test_data[0].keys())\n",
    "TESTSCORE_FIELDS.append('test_score')\n",
    "csv_frames_logger = CSVLogger(os.path.dirname(csv_logger), path_leaf(csv_logger), TESTSCORE_FIELDS)\n",
    "for data in test_data:\n",
    "    data['test_score'] = best_model_test_score\n",
    "    csv_frames_logger.record(data)\n",
    "csv_frames_logger.close()\n",
    "\n",
    "data = pd.DataFrame(test_data)\n",
    "data = data.apply(pd.to_numeric, errors='ignore')\n",
    "data.head(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Check if weights_config better than best_model_weights_config\n",
    "print('BEFORE', best_models_cache)\n",
    "\n",
    "if best_models_cache.get(model_cfg_key, None):\n",
    "    best_model_test_score = 100000\n",
    "    if current_model_test_score > best_model_test_score:       \n",
    "        # Copy weights into new directory (directory_name -> timestamp) under best_models_dir\n",
    "        \n",
    "        # Update cache\n",
    "        print('NEW BEST MODEL')\n",
    "else:\n",
    "    # Assign current weights as 'best_model', copy\n",
    "    # Copy weights into new directory (directory_name -> timestamp) under best_models_dir\n",
    "    best_models_dir = os.path.dirname(best_models_recorder)\n",
    "    d = datetime.datetime.utcnow()\n",
    "    epoch = datetime.datetime(1970,1,1)\n",
    "    best_model_timestamp = int((d - epoch).total_seconds())\n",
    "    best_model_weights_path = os.path.join(best_models_dir, str(best_model_timestamp), path_leaf(weights_config['weights_path']))\n",
    "    os.makedirs(os.path.dirname(best_model_weights_path), exist_ok = True)\n",
    "    shutil.copy(weights_config['weights_path'], best_model_weights_path)\n",
    "    \n",
    "    # Save Labels and Anchors as json\n",
    "    model_cfg = os.path.join(os.path.dirname(best_model_weights_path), 'cfg.json')\n",
    "    with open(model_cfg, 'w+') as fn:\n",
    "        weights_config['weights_path'] = best_model_weights_path\n",
    "        json.dump(weights_config, fn)\n",
    "    \n",
    "    best_models_cache[model_cfg_key] = best_model_weights_path\n",
    "    # Save cache\n",
    "    with open(best_models_recorder, 'wb') as fn:\n",
    "        pickle.dump(best_models_cache, fn)\n",
    "        \n",
    "print('AFTER', best_models_cache)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
