{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import urllib\n",
    "from PIL import Image\n",
    "from enum import Enum\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "import xml.etree.cElementTree as ET\n",
    "import glob\n",
    "import argparse\n",
    "import numpy as np\n",
    "import json\n",
    "import numpy\n",
    "import cv2\n",
    "from collections import OrderedDict\n",
    "import scipy.misc\n",
    "from skimage import measure   \n",
    "from shapely.geometry import Polygon, MultiPolygon, MultiPoint\n",
    "import random\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import shutil\n",
    "import pickle\n",
    "\n",
    "WORKING_DIRECTORY = '/media/dean/datastore/datasets/darknet_evaluate'\n",
    "COCO_DIRECTORY = os.path.join(WORKING_DIRECTORY, 'data', 'coco')\n",
    "BDD10K_COCO_ANNOTATIONS_FILE = os.path.join(COCO_DIRECTORY,'annotations' , 'bdd10k_instances_train2014.json')\n",
    "IMAGES_DIRECTORY = os.path.join(COCO_DIRECTORY, 'images','train2014')\n",
    "WEIGHTS_DIRECTORY = os.path.join(WORKING_DIRECTORY,'trained_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.80s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "bdd10k_coco = COCO(BDD10K_COCO_ANNOTATIONS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112110\n"
     ]
    }
   ],
   "source": [
    "ann_ids = bdd10k_coco.getAnnIds()\n",
    "print(len(ann_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!java -jar $WORKING_DIRECTORY/data/cocotoyolo.jar \"{BDD10K_COCO_ANNOTATIONS_FILE}\" \"{IMAGES_DIRECTORY}/\" \"all\" \"{COCO_DIRECTORY}/labels/train2014\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get All weights files and sort\n",
    "d = {}\n",
    "weights_files = glob.glob(os.path.join(WEIGHTS_DIRECTORY, '*.weights'))\n",
    "\n",
    "# Grab epochs\n",
    "for weights in weights_files:\n",
    "    fname = os.path.split(weights)[-1]\n",
    "    epochs = fname[15:].split('.weights')[0]\n",
    "    d[fname] = int(epochs)\n",
    "    \n",
    "    \n",
    "sorted_weights = OrderedDict(sorted(d.items(), key=lambda kv: kv[1]))\n",
    "bdd_weights = OrderedDict()\n",
    "\n",
    "for weights_file in sorted_weights.keys():\n",
    "    d = {}\n",
    "    d['epochs'] = sorted_weights[weights_file]\n",
    "    d['map_results_file'] = os.path.join(WEIGHTS_DIRECTORY, weights_file+'.txt')\n",
    "    \n",
    "    if not os.path.exists(d['map_results_file']):\n",
    "        outfile = !cd $WORKING_DIRECTORY && ./darknet detector map cfg/bdd100k.data cfg/yolov3-bdd100k.cfg $WEIGHTS_DIRECTORY/$weights_file\n",
    "        with open(d['map_results_file'],\"w+\") as f:\n",
    "            f.write(outfile)    \n",
    "    \n",
    "    #Get MAP results from file\n",
    "    class_stats = []\n",
    "    map_stats = {}\n",
    "    with open(d['map_results_file']) as openfile:\n",
    "        for line in openfile:\n",
    "            for part in line.split():\n",
    "                if 'class_id' in part:\n",
    "                    class_map = {}\n",
    "                    tokens = line.split()\n",
    "                    class_map['class_id'] = int(tokens[2].strip(','))\n",
    "                    class_map['class_name'] = tokens[5].strip(',')\n",
    "                    tokens = line.split('ap = ')\n",
    "                    class_map['class_ap'] = tokens[1].split()[0].strip(',')\n",
    "\n",
    "                    class_stats.append(class_map)\n",
    "                elif '(mAP)' in part:\n",
    "                    tokens = line.split('(mAP) =')[1]\n",
    "                    map_stats['mean_avg_precision'] = tokens.split()[2].strip(',')\n",
    "            if 'Total Detection Time:' in line:\n",
    "                tokens = line.split(':')[1]\n",
    "                total_detection_time = float(tokens.split()[0])\n",
    "                map_stats['total_detection_time'] = total_detection_time\n",
    "    map_stats['class_stats'] = class_stats\n",
    "    print(map_stats) \n",
    "    bdd_weights[weights] = map_stats\n",
    "\n",
    "    \n",
    "    \n",
    "# Make sure Dictionary is made properly\n",
    "print(bdd_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_stats = []\n",
    "map_stats = {}\n",
    "with open(\"/media/dean/datastore/datasets/darknet_evaluate/yolov3-bdd100k_24864.weights.txt\") as openfile:\n",
    "    for line in openfile:\n",
    "        for part in line.split():\n",
    "            if 'class_id' in part:\n",
    "                class_map = {}\n",
    "                tokens = line.split()\n",
    "                class_map['class_id'] = int(tokens[2].strip(','))\n",
    "                class_map['class_name'] = tokens[5].strip(',')\n",
    "                tokens = line.split('ap = ')\n",
    "                class_map['class_ap'] = tokens[1].split()[0].strip(',')\n",
    "                \n",
    "                class_stats.append(class_map)\n",
    "            elif '(mAP)' in part:\n",
    "                tokens = line.split('(mAP) =')[1]\n",
    "                map_stats['mean_avg_precision'] = tokens.split()[2].strip(',')\n",
    "        if 'Total Detection Time:' in line:\n",
    "            tokens = line.split(':')[1]\n",
    "            total_detection_time = float(tokens.split()[0])\n",
    "            map_stats['total_detection_time'] = total_detection_time\n",
    "map_stats['class_stats'] = class_stats\n",
    "print(map_stats)         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #initialize COCO ground truth api\n",
    "# cocoGt=COCO(BDD10K_COCO_ANNOTATIONS_FILE)\n",
    "\n",
    "# bdd10k_coco_results = []\n",
    "\n",
    "# for results_file in bdd10k_coco_results:\n",
    "#     #initialize COCO detections api\n",
    "#     resFile='/home/dean/Desktop/coco_results.json'\n",
    "#     cocoDt=cocoGt.loadRes(resFile)\n",
    "\n",
    "#     for cat in category_names:\n",
    "#         category_ids = cocoDt.getCatIds(catNms=cat)\n",
    "#         imgIds=cocoDt.getImgIds(catIds=category_ids)\n",
    "#         print('MAP for:', cat)\n",
    "#         # running evaluation\n",
    "#         cocoEval = COCOeval(cocoGt,cocoDt,annType)\n",
    "#         cocoEval.params.imgIds  = imgIds\n",
    "#         cocoEval.evaluate()\n",
    "#         cocoEval.accumulate()\n",
    "#         cocoEval.summarize()\n",
    "#         print('\\n\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ros-kache]",
   "language": "python",
   "name": "conda-env-ros-kache-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
