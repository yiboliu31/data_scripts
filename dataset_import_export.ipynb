{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c48d16b3-ef09-4a56-8622-3506089cd0cf\n",
      "5419f85a-d1b4-4a39-a786-e702c0930764.zip\n",
      "IN PROCESS\n",
      "None\n",
      "SUCCESS\n",
      "7dcb6481-f871-4852-9e54-b66d5c264180\n",
      "8dd0e0e3-a3ee-4f44-98ee-84922324dab2.zip\n",
      "IN PROCESS\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "SUCCESS\n",
      "e6096fbe-e13f-4b33-9407-5203ba3c6507\n",
      "46760dc4-244f-468d-acc1-68649ac885e6.zip\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "SUCCESS\n",
      "f644e975-fa63-4007-a05e-c305f1e7ce3b\n",
      "a0b27d33-2b8e-4b0e-a470-dde22828b83a.zip\n",
      "None\n",
      "None\n",
      "SUCCESS\n",
      "ba708b1c-1990-415e-b3e6-0f1c170b23f8\n",
      "b8d61b4d-65f7-46bb-9880-21100acb4fbd.zip\n",
      "IN PROCESS\n",
      "None\n",
      "None\n",
      "SUCCESS\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import ntpath\n",
    "import json\n",
    "import os\n",
    "from pipelineapi import PipelineAPI\n",
    "import time\n",
    "\n",
    "\n",
    "def get_dataset(ds, data_pipeline_api = None):\n",
    "    if not data_pipeline_api:\n",
    "        data_pipeline_api = PipelineAPI(\"http://ec2-54-184-229-238.us-west-2.compute.amazonaws.com/\", 'admin@pronto.ai', 'admin123!')\n",
    "\n",
    "\n",
    "    dataset_names = ['pronto_set', 'bdd_set', 'kache_set','china_set']\n",
    "    os.makedirs(kache_anns_dir, exist_ok = True)\n",
    "\n",
    "    task, file_name = data_pipeline_api.data_file(video_source_names=[], dataset_names=[ds],\n",
    "                                categories_names=[], label_statuses=[], bulk_size=6000)\n",
    "    return task, file_name\n",
    "\n",
    "dataset_names = ['pronto_set', 'bdd_set', 'kache_set','china_set', 'holdout_set']\n",
    "data_pipeline_api = PipelineAPI(\"http://ec2-54-184-229-238.us-west-2.compute.amazonaws.com/\", 'admin@pronto.ai', 'admin123!')\n",
    "kache_anns_dir =  '/home/kuser/Desktop/all_sets_for_barrier_labels/'\n",
    "os.makedirs(kache_anns_dir ,exist_ok=True)\n",
    "for ds in dataset_names:\n",
    "    \n",
    "    task, filename = get_dataset(ds, data_pipeline_api)\n",
    "    print(task); print(filename)\n",
    "\n",
    "    while not os.path.exists(os.path.join(kache_anns_dir, os.path.splitext(filename)[0]+'_1.json')):\n",
    "        print(data_pipeline_api.get_file(task_id=task, folder=kache_anns_dir))\n",
    "        time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import ntpath\n",
    "import glob\n",
    "\n",
    "\n",
    "def path_leaf(path):\n",
    "    if urllib.parse.urlparse(path).scheme != \"\" or os.path.isabs(path):\n",
    "        path = os.path.split(path)[-1]\n",
    "    head, tail = ntpath.split(path)\n",
    "    return tail or ntpath.basename(head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k_file in glob.glob('/home/kuser/Desktop/all_sets_for_barrier_labels/*.json'):\n",
    "    with open(k_file) as f:\n",
    "        data = json.load(f)\n",
    "    for kache_img_data in data:\n",
    "        kache_img_data['labels'] = []\n",
    "        \n",
    "    with open(k_file, 'w') as fn:\n",
    "        json.dump(data, fn)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k_file in glob.glob('/home/kuser/Desktop/all_sets_for_barrier_labels/*.json'):\n",
    "    with open(k_file) as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    scalabel_data_format = []\n",
    "    total_idx = 0\n",
    "    key_mappings  = {} #kache_id-> scalabel_id\n",
    "    # Get Pending Precision Labels\n",
    "    \n",
    "    for frame in data:\n",
    "        img_name = path_leaf(frame['name'])    \n",
    "\n",
    "        scalabel_frame = {\n",
    "            'url': frame['url'],\n",
    "            'name': frame['name'],\n",
    "            'width': frame['width'],\n",
    "            'height': frame['height'],\n",
    "            'labels': [],\n",
    "            'index': total_idx,\n",
    "            'timestamp': frame['timestamp'],\n",
    "            'videoName': '',\n",
    "            'attributes': {}\n",
    "        }\n",
    "        frame['scalabel_id'] = total_idx\n",
    "        frame['index'] = total_idx\n",
    "        scalabel_data_format.append(scalabel_frame)\n",
    "        total_idx += 1\n",
    "        \n",
    "    with open(k_file, 'w+') as fn:\n",
    "        json.dump(data, fn)\n",
    "        \n",
    "    with open(os.path.join('/home/kuser/Desktop/all_sets_for_barrier_labels/scalabel/', path_leaf(k_file)), 'w+') as fn:\n",
    "        json.dump(scalabel_data_format, fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import zip_longest\n",
    "import urllib.parse as urlparse\n",
    "from urllib.parse import urlencode\n",
    "    \n",
    "def push_to_scalabel(scalabel_file):\n",
    "    with open(\"/media/kuser/datastore/nebula/pipelines/configs/entanglement_cfg.tml\", 'r') as tmlfile:\n",
    "        cfg = toml.load(tmlfile)\n",
    "\n",
    "\n",
    "    args = cfg['scalabel']['args']\n",
    "    scalabel_api_uri = \"{}postProject?{}\".format(\n",
    "        cfg['scalabel']['base_url'], urlencode(args))\n",
    "\n",
    "    project_name = os.path.splitext(path_leaf(scalabel_file))[0]\n",
    "\n",
    "\n",
    "    body = {\"project_name\": project_name,\n",
    "            \"item_type\": cfg['scalabel']['item_type'],\n",
    "            \"label_type\": cfg['scalabel']['label_type'],\n",
    "            \"page_title\": \"Label Missing Barriers\",\n",
    "            \"task_size\": \"6000\",\n",
    "            \"vendor_id\": str(cfg['scalabel']['vendor_id']),\n",
    "            }\n",
    "\n",
    "    files = {'item_file': open(scalabel_file, 'rb'),\n",
    "             'categories': open(cfg['scalabel']['categories_file'], 'rb'),\n",
    "             'attributes': open(cfg['scalabel']['attributes_file'], 'rb')}\n",
    "\n",
    "    try:\n",
    "        print('Initializing Project: {}'.format(project_name))\n",
    "        resp = requests.post(scalabel_api_uri, files=files, data=body)\n",
    "    except requests.exceptions.Timeout as e1:\n",
    "        # Maybe set up for a retry, or continue in a retry loop\n",
    "        print(e1)\n",
    "    except requests.exceptions.TooManyRedirects as e2:\n",
    "        # Tell the user their URL was bad and try a different one\n",
    "        print(e2)\n",
    "    except requests.exceptions.RequestException as e3:\n",
    "        # catastrophic error. bail.\n",
    "        print(e3)\n",
    "        sys.exit(1)\n",
    "\n",
    "    if resp.status_code != 200 or resp.content != ''.encode('UTF-8'):\n",
    "        raise requests.exceptions.RequestException(\n",
    "            'POST /create status code: {}, content {}'.format(resp.status_code, resp.content))\n",
    "\n",
    "    if body['label_type'] == 'box2d' or body['label_type'] == 'lane':\n",
    "        lbl_type = 'label2d'\n",
    "\n",
    "    active_precision_uri = f\"{cfg['scalabel']['base_url']}{lbl_type}?project_name={project_name}&task_index=0\"\n",
    "    print('Successfully Created Project. URL:', active_precision_uri)\n",
    "    return active_precision_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Project: 8dd0e0e3-a3ee-4f44-98ee-84922324dab2_10\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=8dd0e0e3-a3ee-4f44-98ee-84922324dab2_10&task_index=0\n",
      "Initializing Project: b8d61b4d-65f7-46bb-9880-21100acb4fbd_3\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=b8d61b4d-65f7-46bb-9880-21100acb4fbd_3&task_index=0\n",
      "Initializing Project: b8d61b4d-65f7-46bb-9880-21100acb4fbd_4\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=b8d61b4d-65f7-46bb-9880-21100acb4fbd_4&task_index=0\n",
      "Initializing Project: a0b27d33-2b8e-4b0e-a470-dde22828b83a_1\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=a0b27d33-2b8e-4b0e-a470-dde22828b83a_1&task_index=0\n",
      "Initializing Project: 8dd0e0e3-a3ee-4f44-98ee-84922324dab2_22\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=8dd0e0e3-a3ee-4f44-98ee-84922324dab2_22&task_index=0\n",
      "Initializing Project: 8dd0e0e3-a3ee-4f44-98ee-84922324dab2_24\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=8dd0e0e3-a3ee-4f44-98ee-84922324dab2_24&task_index=0\n",
      "Initializing Project: 8dd0e0e3-a3ee-4f44-98ee-84922324dab2_21\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=8dd0e0e3-a3ee-4f44-98ee-84922324dab2_21&task_index=0\n",
      "Initializing Project: 46760dc4-244f-468d-acc1-68649ac885e6_6\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=46760dc4-244f-468d-acc1-68649ac885e6_6&task_index=0\n",
      "Initializing Project: 8dd0e0e3-a3ee-4f44-98ee-84922324dab2_16\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=8dd0e0e3-a3ee-4f44-98ee-84922324dab2_16&task_index=0\n",
      "Initializing Project: 46760dc4-244f-468d-acc1-68649ac885e6_3\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=46760dc4-244f-468d-acc1-68649ac885e6_3&task_index=0\n",
      "Initializing Project: 46760dc4-244f-468d-acc1-68649ac885e6_11\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=46760dc4-244f-468d-acc1-68649ac885e6_11&task_index=0\n",
      "Initializing Project: 5419f85a-d1b4-4a39-a786-e702c0930764_16\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=5419f85a-d1b4-4a39-a786-e702c0930764_16&task_index=0\n",
      "Initializing Project: a0b27d33-2b8e-4b0e-a470-dde22828b83a_2\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=a0b27d33-2b8e-4b0e-a470-dde22828b83a_2&task_index=0\n",
      "Initializing Project: 5419f85a-d1b4-4a39-a786-e702c0930764_15\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=5419f85a-d1b4-4a39-a786-e702c0930764_15&task_index=0\n",
      "Initializing Project: 8dd0e0e3-a3ee-4f44-98ee-84922324dab2_6\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=8dd0e0e3-a3ee-4f44-98ee-84922324dab2_6&task_index=0\n",
      "Initializing Project: 46760dc4-244f-468d-acc1-68649ac885e6_7\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=46760dc4-244f-468d-acc1-68649ac885e6_7&task_index=0\n",
      "Initializing Project: b8d61b4d-65f7-46bb-9880-21100acb4fbd_1\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=b8d61b4d-65f7-46bb-9880-21100acb4fbd_1&task_index=0\n",
      "Initializing Project: 5419f85a-d1b4-4a39-a786-e702c0930764_1\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=5419f85a-d1b4-4a39-a786-e702c0930764_1&task_index=0\n",
      "Initializing Project: 5419f85a-d1b4-4a39-a786-e702c0930764_5\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=5419f85a-d1b4-4a39-a786-e702c0930764_5&task_index=0\n",
      "Initializing Project: 5419f85a-d1b4-4a39-a786-e702c0930764_13\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=5419f85a-d1b4-4a39-a786-e702c0930764_13&task_index=0\n",
      "Initializing Project: 8dd0e0e3-a3ee-4f44-98ee-84922324dab2_15\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=8dd0e0e3-a3ee-4f44-98ee-84922324dab2_15&task_index=0\n",
      "Initializing Project: 8dd0e0e3-a3ee-4f44-98ee-84922324dab2_9\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=8dd0e0e3-a3ee-4f44-98ee-84922324dab2_9&task_index=0\n",
      "Initializing Project: 8dd0e0e3-a3ee-4f44-98ee-84922324dab2_2\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=8dd0e0e3-a3ee-4f44-98ee-84922324dab2_2&task_index=0\n",
      "Initializing Project: 8dd0e0e3-a3ee-4f44-98ee-84922324dab2_7\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=8dd0e0e3-a3ee-4f44-98ee-84922324dab2_7&task_index=0\n",
      "Initializing Project: 46760dc4-244f-468d-acc1-68649ac885e6_9\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=46760dc4-244f-468d-acc1-68649ac885e6_9&task_index=0\n",
      "Initializing Project: 8dd0e0e3-a3ee-4f44-98ee-84922324dab2_23\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=8dd0e0e3-a3ee-4f44-98ee-84922324dab2_23&task_index=0\n",
      "Initializing Project: 5419f85a-d1b4-4a39-a786-e702c0930764_10\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=5419f85a-d1b4-4a39-a786-e702c0930764_10&task_index=0\n",
      "Initializing Project: 5419f85a-d1b4-4a39-a786-e702c0930764_3\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=5419f85a-d1b4-4a39-a786-e702c0930764_3&task_index=0\n",
      "Initializing Project: 8dd0e0e3-a3ee-4f44-98ee-84922324dab2_12\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=8dd0e0e3-a3ee-4f44-98ee-84922324dab2_12&task_index=0\n",
      "Initializing Project: 8dd0e0e3-a3ee-4f44-98ee-84922324dab2_5\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=8dd0e0e3-a3ee-4f44-98ee-84922324dab2_5&task_index=0\n",
      "Initializing Project: b8d61b4d-65f7-46bb-9880-21100acb4fbd_5\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=b8d61b4d-65f7-46bb-9880-21100acb4fbd_5&task_index=0\n",
      "Initializing Project: 8dd0e0e3-a3ee-4f44-98ee-84922324dab2_1\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=8dd0e0e3-a3ee-4f44-98ee-84922324dab2_1&task_index=0\n",
      "Initializing Project: 5419f85a-d1b4-4a39-a786-e702c0930764_8\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=5419f85a-d1b4-4a39-a786-e702c0930764_8&task_index=0\n",
      "Initializing Project: 8dd0e0e3-a3ee-4f44-98ee-84922324dab2_25\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=8dd0e0e3-a3ee-4f44-98ee-84922324dab2_25&task_index=0\n",
      "Initializing Project: 8dd0e0e3-a3ee-4f44-98ee-84922324dab2_17\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=8dd0e0e3-a3ee-4f44-98ee-84922324dab2_17&task_index=0\n",
      "Initializing Project: 5419f85a-d1b4-4a39-a786-e702c0930764_6\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=5419f85a-d1b4-4a39-a786-e702c0930764_6&task_index=0\n",
      "Initializing Project: a0b27d33-2b8e-4b0e-a470-dde22828b83a_3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=a0b27d33-2b8e-4b0e-a470-dde22828b83a_3&task_index=0\n",
      "Initializing Project: 5419f85a-d1b4-4a39-a786-e702c0930764_11\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=5419f85a-d1b4-4a39-a786-e702c0930764_11&task_index=0\n",
      "Initializing Project: 8dd0e0e3-a3ee-4f44-98ee-84922324dab2_14\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=8dd0e0e3-a3ee-4f44-98ee-84922324dab2_14&task_index=0\n",
      "Initializing Project: 46760dc4-244f-468d-acc1-68649ac885e6_5\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=46760dc4-244f-468d-acc1-68649ac885e6_5&task_index=0\n",
      "Initializing Project: 5419f85a-d1b4-4a39-a786-e702c0930764_7\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=5419f85a-d1b4-4a39-a786-e702c0930764_7&task_index=0\n",
      "Initializing Project: 5419f85a-d1b4-4a39-a786-e702c0930764_12\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=5419f85a-d1b4-4a39-a786-e702c0930764_12&task_index=0\n",
      "Initializing Project: 46760dc4-244f-468d-acc1-68649ac885e6_4\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=46760dc4-244f-468d-acc1-68649ac885e6_4&task_index=0\n",
      "Initializing Project: 5419f85a-d1b4-4a39-a786-e702c0930764_9\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=5419f85a-d1b4-4a39-a786-e702c0930764_9&task_index=0\n",
      "Initializing Project: 5419f85a-d1b4-4a39-a786-e702c0930764_2\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=5419f85a-d1b4-4a39-a786-e702c0930764_2&task_index=0\n",
      "Initializing Project: 5419f85a-d1b4-4a39-a786-e702c0930764_17\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=5419f85a-d1b4-4a39-a786-e702c0930764_17&task_index=0\n",
      "Initializing Project: 5419f85a-d1b4-4a39-a786-e702c0930764_14\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=5419f85a-d1b4-4a39-a786-e702c0930764_14&task_index=0\n",
      "Initializing Project: 8dd0e0e3-a3ee-4f44-98ee-84922324dab2_3\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=8dd0e0e3-a3ee-4f44-98ee-84922324dab2_3&task_index=0\n",
      "Initializing Project: 8dd0e0e3-a3ee-4f44-98ee-84922324dab2_11\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=8dd0e0e3-a3ee-4f44-98ee-84922324dab2_11&task_index=0\n",
      "Initializing Project: 46760dc4-244f-468d-acc1-68649ac885e6_1\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=46760dc4-244f-468d-acc1-68649ac885e6_1&task_index=0\n",
      "Initializing Project: 8dd0e0e3-a3ee-4f44-98ee-84922324dab2_18\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=8dd0e0e3-a3ee-4f44-98ee-84922324dab2_18&task_index=0\n",
      "Initializing Project: 46760dc4-244f-468d-acc1-68649ac885e6_2\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=46760dc4-244f-468d-acc1-68649ac885e6_2&task_index=0\n",
      "Initializing Project: 8dd0e0e3-a3ee-4f44-98ee-84922324dab2_8\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=8dd0e0e3-a3ee-4f44-98ee-84922324dab2_8&task_index=0\n",
      "Initializing Project: 8dd0e0e3-a3ee-4f44-98ee-84922324dab2_20\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=8dd0e0e3-a3ee-4f44-98ee-84922324dab2_20&task_index=0\n",
      "Initializing Project: b8d61b4d-65f7-46bb-9880-21100acb4fbd_2\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=b8d61b4d-65f7-46bb-9880-21100acb4fbd_2&task_index=0\n",
      "Initializing Project: 46760dc4-244f-468d-acc1-68649ac885e6_8\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=46760dc4-244f-468d-acc1-68649ac885e6_8&task_index=0\n",
      "Initializing Project: 46760dc4-244f-468d-acc1-68649ac885e6_10\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=46760dc4-244f-468d-acc1-68649ac885e6_10&task_index=0\n",
      "Initializing Project: 8dd0e0e3-a3ee-4f44-98ee-84922324dab2_13\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=8dd0e0e3-a3ee-4f44-98ee-84922324dab2_13&task_index=0\n",
      "Initializing Project: 8dd0e0e3-a3ee-4f44-98ee-84922324dab2_4\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=8dd0e0e3-a3ee-4f44-98ee-84922324dab2_4&task_index=0\n",
      "Initializing Project: 5419f85a-d1b4-4a39-a786-e702c0930764_4\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=5419f85a-d1b4-4a39-a786-e702c0930764_4&task_index=0\n",
      "Initializing Project: 8dd0e0e3-a3ee-4f44-98ee-84922324dab2_19\n",
      "Successfully Created Project. URL: http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name=8dd0e0e3-a3ee-4f44-98ee-84922324dab2_19&task_index=0\n"
     ]
    }
   ],
   "source": [
    "import toml\n",
    "import argparse\n",
    "import requests\n",
    "\n",
    "for k_file in glob.glob('/home/kuser/Desktop/all_sets_for_barrier_labels/scalabel/*.json'):\n",
    "    push_to_scalabel(k_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipelineapi import PipelineAPI\n",
    "import os\n",
    "\n",
    "def dataset_import(dataset_name, dataset_file):\n",
    "        data_pipeline_api = PipelineAPI(\"http://ec2-54-149-236-75.us-west-2.compute.amazonaws.com/\", 'admin@pronto.ai', 'admin123!')\n",
    "        with open(dataset_file, 'rb') as f:\n",
    "            data_pipeline_api.upload_json_file(f, file_name=path_leaf(dataset_file), round='inspection', dataset_name=dataset_name, skip_image=True)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipelineapi import PipelineAPI\n",
    "import os\n",
    "\n",
    "def dataset_import(dataset_name, dataset_file):\n",
    "        data_pipeline_api = PipelineAPI(\"http://ec2-54-191-65-138.us-west-2.compute.amazonaws.com/\", 'admin@pronto.ai', 'admin123!')\n",
    "        with open(dataset_file, 'rb') as f:\n",
    "            data_pipeline_api.upload_json_file(f, file_name=path_leaf(dataset_file), round='inspection', dataset_name=dataset_name, skip_image=False)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import ntpath\n",
    "import json\n",
    "import os\n",
    "from pipelineapi import PipelineAPI\n",
    "\n",
    "def get_dataset():\n",
    "    data_pipeline_api = PipelineAPI(\"http://ec2-54-184-229-238.us-west-2.compute.amazonaws.com/\", 'admin@pronto.ai', 'admin123!')\n",
    "\n",
    "    batch_size = 0\n",
    "    batch = 0\n",
    "    batch_anns = []\n",
    "\n",
    "\n",
    "    dataset_names = ['holdout_set']\n",
    "    kache_anns_dir =  '/home/dean/Desktop/pronto_set/'\n",
    "    os.makedirs(kache_anns_dir, exist_ok = True)\n",
    "\n",
    "    databunch = data_pipeline_api.data(dataset_names=dataset_names, video_source_names=[], categories_names=[],\n",
    "                    label_statuses=[42], page_size=None, frame_id=None) # returns generator\n",
    "\n",
    "    for anns in databunch:\n",
    "#           anns = update_traffic_light_cats([anns])\n",
    "        batch_anns.append(anns)\n",
    "        batch_size += len(anns)\n",
    "        if batch_size % 250001 == 0:\n",
    "            with open(os.path.join(kache_anns_dir,f'{dataset_names[0]}--accuracy-rnd.json'), 'w') as fn:\n",
    "                json.dump(batch_anns, fn)\n",
    "            batch_anns = []\n",
    "            batch += 1\n",
    "\n",
    "    # Save any remaining frames\n",
    "    if len(batch_anns) != 0:\n",
    "        with open(os.path.join(kache_anns_dir,f'{dataset_names[0]}--accuracy-rnd.json'), 'w') as fn:\n",
    "            json.dump(batch_anns, fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import ntpath\n",
    "from pipelineapi import PipelineAPI\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "\n",
    "\n",
    "def path_leaf(path):\n",
    "    if urllib.parse.urlparse(path).scheme != \"\" or os.path.isabs(path):\n",
    "        path = os.path.split(path)[-1]\n",
    "    head, tail = ntpath.split(path)\n",
    "    return tail or ntpath.basename(head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1562750429.150900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p_file in glob.glob('/media/dean/datastore/nebula/pipelines/kache/pulsar/precision/active_precision_projects/*.json'):\n",
    "    with open(p_file) as f:\n",
    "        precision_data = json.load(f)\n",
    "    for kache_id, kache_img_data in precision_data.items():\n",
    "        for lbl in kache_img_data['labels']:\n",
    "            print(f\"File: {p_file} Scalabel Label Id {lbl['scalabel_label_id']} kache label id: {lbl['kache_label_id']}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matching 0 with 3021857\n",
      "matching 1 with 3021858\n",
      "matching 2 with 3021860\n",
      "matching 3 with 3021861\n",
      "matching 4 with 3021862\n",
      "matching 5 with 3021863\n",
      "matching 6 with 3021864\n",
      "matching 7 with 3021859\n",
      "matching 8 with 3022160\n",
      "matching 9 with 3022161\n",
      "matching 10 with 3022162\n",
      "matching 11 with 3022164\n",
      "matching 12 with 3022165\n",
      "matching 13 with 3022166\n",
      "matching 14 with 3022167\n",
      "matching 15 with 3022168\n",
      "matching 16 with 3022169\n",
      "matching 17 with 3022163\n",
      "matching 18 with 3022874\n",
      "matching 19 with 3022876\n",
      "matching 20 with 3022877\n",
      "matching 21 with 3022875\n",
      "matching 22 with 3022882\n",
      "matching 23 with 3022884\n",
      "matching 24 with 3022885\n",
      "matching 25 with 3022886\n",
      "matching 26 with 3022887\n",
      "matching 27 with 3022888\n",
      "matching 28 with 3022889\n",
      "matching 29 with 3022890\n",
      "matching 30 with 3022891\n",
      "matching 31 with 3022883\n",
      "matching 32 with 3022892\n",
      "matching 33 with 3022893\n",
      "matching 34 with 3022894\n",
      "matching 35 with 3023086\n",
      "matching 36 with 3023087\n",
      "matching 37 with 3023088\n",
      "matching 38 with 3023089\n",
      "matching 39 with 3023090\n",
      "matching 40 with 3023092\n",
      "matching 41 with 3023093\n",
      "matching 42 with 3023094\n",
      "matching 43 with 3023095\n",
      "matching 44 with 3023096\n",
      "matching 45 with 3023091\n",
      "matching 46 with 3023559\n",
      "matching 47 with 3023560\n",
      "matching 48 with 3023561\n",
      "matching 49 with 3023562\n",
      "matching 50 with 3023564\n",
      "matching 51 with 3023565\n",
      "matching 52 with 3023566\n",
      "matching 53 with 3023567\n",
      "matching 54 with 3023568\n",
      "matching 55 with 3023569\n",
      "matching 56 with 3023570\n",
      "matching 57 with 3023572\n",
      "matching 58 with 3023573\n",
      "matching 59 with 3023574\n",
      "matching 60 with 3023575\n",
      "matching 61 with 3023576\n",
      "matching 62 with 3023577\n",
      "matching 63 with 3023578\n",
      "matching 64 with 3023580\n",
      "matching 65 with 3023581\n",
      "matching 66 with 3023582\n",
      "matching 67 with 3023583\n",
      "matching 68 with 3023584\n",
      "matching 69 with 3023585\n",
      "matching 70 with 3023563\n",
      "matching 71 with 3023571\n",
      "matching 72 with 3023579\n",
      "matching 73 with 3024164\n",
      "matching 74 with 3024165\n",
      "matching 75 with 3024166\n",
      "matching 76 with 3024167\n",
      "matching 77 with 3024168\n",
      "matching 78 with 3024169\n",
      "matching 79 with 3024170\n",
      "matching 80 with 3024171\n",
      "matching 81 with 3024172\n",
      "matching 82 with 3024173\n",
      "matching 83 with 3024174\n",
      "matching 84 with 3024175\n",
      "matching 85 with 3024176\n",
      "matching 86 with 3024177\n",
      "matching 87 with 3024178\n",
      "matching 88 with 3024180\n",
      "matching 89 with 3024179\n",
      "matching 90 with 3024181\n",
      "matching 91 with 3024182\n",
      "matching 92 with 3024183\n",
      "matching 93 with 3024184\n",
      "matching 94 with 3024185\n",
      "matching 95 with 3024186\n",
      "matching 96 with 3024187\n",
      "matching 97 with 3024456\n",
      "matching 98 with 3024457\n",
      "matching 99 with 3024458\n",
      "matching 100 with 3024460\n",
      "matching 101 with 3024461\n",
      "matching 102 with 3024459\n",
      "matching 103 with 3024590\n",
      "matching 104 with 3024591\n",
      "matching 105 with 3024592\n",
      "matching 106 with 3024593\n",
      "matching 107 with 3024629\n",
      "matching 108 with 3024630\n",
      "matching 109 with 3024631\n",
      "matching 110 with 3024696\n",
      "matching 111 with 3024697\n",
      "matching 112 with 3024698\n",
      "matching 113 with 3024700\n",
      "matching 114 with 3024701\n",
      "matching 115 with 3024702\n",
      "matching 116 with 3024703\n",
      "matching 117 with 3024704\n",
      "matching 118 with 3024705\n",
      "matching 119 with 3024706\n",
      "matching 120 with 3024699\n",
      "matching 121 with 3024908\n",
      "matching 122 with 3024909\n",
      "matching 123 with 3024910\n",
      "matching 124 with 3024911\n",
      "matching 125 with 3024912\n",
      "matching 126 with 3024913\n",
      "matching 127 with 3024914\n",
      "matching 128 with 3024916\n",
      "matching 129 with 3024917\n",
      "matching 130 with 3024918\n",
      "matching 131 with 3024919\n",
      "matching 132 with 3024920\n",
      "matching 133 with 3024921\n",
      "matching 134 with 3024915\n",
      "matching 135 with 3024907\n",
      "matching 136 with 3024972\n",
      "matching 137 with 3024973\n",
      "matching 138 with 3024974\n",
      "matching 139 with 3024975\n",
      "matching 140 with 3024976\n",
      "matching 141 with 3024977\n",
      "matching 142 with 3024978\n",
      "matching 143 with 3024980\n",
      "matching 144 with 3024981\n",
      "matching 145 with 3024982\n",
      "matching 146 with 3024983\n",
      "matching 147 with 3024984\n",
      "matching 148 with 3024985\n",
      "matching 149 with 3024986\n",
      "matching 150 with 3024988\n",
      "matching 151 with 3024971\n",
      "matching 152 with 3024979\n",
      "matching 153 with 3024987\n",
      "matching 154 with 3025150\n",
      "matching 155 with 3025151\n",
      "matching 156 with 3025152\n",
      "matching 157 with 3025153\n",
      "matching 158 with 3025154\n",
      "matching 159 with 3025155\n",
      "matching 160 with 3025300\n",
      "matching 161 with 3025301\n",
      "matching 162 with 3025302\n",
      "matching 163 with 3025303\n",
      "matching 164 with 3025304\n",
      "matching 165 with 3025305\n",
      "matching 166 with 3025306\n",
      "matching 167 with 3025308\n",
      "matching 168 with 3025309\n",
      "matching 169 with 3025310\n",
      "matching 170 with 3025311\n",
      "matching 171 with 3025312\n",
      "matching 172 with 3025307\n",
      "matching 173 with 3025337\n",
      "matching 174 with 3025338\n",
      "matching 175 with 3025340\n",
      "matching 176 with 3025341\n",
      "matching 177 with 3025342\n",
      "matching 178 with 3025343\n",
      "matching 179 with 3025344\n",
      "matching 180 with 3025345\n",
      "matching 181 with 3025346\n",
      "matching 182 with 3025348\n",
      "matching 183 with 3025339\n",
      "matching 184 with 3025347\n",
      "matching 185 with 3025452\n",
      "matching 186 with 3025453\n",
      "matching 187 with 3025454\n",
      "matching 188 with 3025455\n",
      "matching 189 with 3025456\n",
      "matching 190 with 3025457\n",
      "matching 191 with 3025458\n",
      "matching 192 with 3025460\n",
      "matching 193 with 3025461\n",
      "matching 194 with 3025462\n",
      "matching 195 with 3025463\n",
      "matching 196 with 3025464\n",
      "matching 197 with 3025465\n",
      "matching 198 with 3025466\n",
      "matching 199 with 3025459\n",
      "matching 200 with 3026378\n",
      "matching 201 with 3026377\n",
      "matching 202 with 3026376\n",
      "matching 203 with 3026375\n",
      "matching 204 with 3026374\n",
      "matching 205 with 3026373\n",
      "matching 206 with 3026372\n",
      "matching 207 with 3026371\n",
      "matching 208 with 3026370\n",
      "matching 209 with 3026369\n",
      "matching 210 with 3026368\n",
      "matching 211 with 3026367\n",
      "matching 212 with 3026366\n",
      "matching 213 with 3026365\n",
      "matching 214 with 3026364\n",
      "matching 215 with 3026363\n",
      "matching 216 with 3026362\n",
      "matching 217 with 3026467\n",
      "matching 218 with 3026466\n",
      "matching 219 with 3026465\n",
      "matching 220 with 3026464\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "hfile = '/home/dean/Desktop/pronto_set/holdout_set--accuracy-rnd.json'\n",
    "with open(hfile) as f:\n",
    "    precision_data3 = json.load(f)\n",
    "    \n",
    "precision_data =precision_data3\n",
    "\n",
    "\n",
    "urls = defaultdict(list)\n",
    "for sfile in glob.glob('/home/dean/Desktop/pronto_set/scalabel_files/*.json'):\n",
    "    with open(sfile) as f:\n",
    "        data = json.load(f)\n",
    "                 \n",
    "    for img_data in data[\"Task\"][\"items\"]:\n",
    "        found= False\n",
    "        for kache_img_data in precision_data:\n",
    "            if kache_img_data['url'] == img_data['url']:\n",
    "                kache_img_data['scalabel_id'] = img_data['index']\n",
    "                kache_img_data['index'] = img_data['index']\n",
    "                \n",
    "                # Match labels\n",
    "                if img_data.get(\"labelIds\"):\n",
    "                    for lbl_idx in sorted(img_data[\"labelIds\"]):\n",
    "                        scalabel_lbl = [lb for lb in data[\"Labels\"] if lb['id'] == lbl_idx][0]\n",
    "                        # Find kache label\n",
    "                        if kache_img_data.get(\"labels\"):\n",
    "                            for lbl in kache_img_data['labels']:\n",
    "                                scanns = np.array([scalabel_lbl['data']['x'], scalabel_lbl['data']['y'], scalabel_lbl['data']['x'] + scalabel_lbl['data']['w'] -0.5 , scalabel_lbl['data']['y'] + scalabel_lbl['data']['h'] -0.5])\n",
    "                                if lbl.get('box2d',None):\n",
    "                                    kanns = np.array([lbl['box2d']['x1'], lbl['box2d']['y1'], lbl['box2d']['x2'], lbl['box2d']['y2']])\n",
    "                                elif lbl.get('category',None) in ['lane', 'drivable area']:\n",
    "                                    lbl['label_status'] = 31\n",
    "                                    continue\n",
    "                                else:\n",
    "                                    kanns = np.array([0,0,0,0])\n",
    "                                        \n",
    "                                if np.allclose(scanns, kanns, atol=6.5):\n",
    "                                    print(f\"matching {scalabel_lbl['id']} with {lbl['kache_label_id']}\")\n",
    "                                    lbl['scalabel_label_id'] = scalabel_lbl['id']\n",
    "                                \n",
    "                \n",
    "                \n",
    "                \n",
    "                urls[path_leaf(sfile)].append(kache_img_data)\n",
    "                found = True\n",
    "                break\n",
    "                    \n",
    "            if found: break\n",
    "    fname = os.path.splitext(path_leaf(sfile))[0]\n",
    "    with open(f\"/home/dean/Desktop/kache_files/{fname}.json\", \"w+\") as f:\n",
    "        json.dump(list(urls[path_leaf(sfile)]), f)     \n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def found_deleted_label(kache_lbl_idx, deleted_lbls):\n",
    "    found = False\n",
    "    del_lbl = None\n",
    "    for lbl_idx, lbl in deleted_lbls.items():\n",
    "        if kache_lbl_idx.get('scalabel_label_id', None):\n",
    "            if kache_lbl_idx['scalabel_label_id'] == lbl_idx:\n",
    "                found = True\n",
    "                del_lbl = lbl\n",
    "                return found, del_lbl\n",
    "    \n",
    "    return found, del_lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def update_kache_label(kache_lbl_idx, found_lbls):\n",
    "    for lbl_idx, lb_dict in found_lbls.items():\n",
    "        scalabel_lbl = lb_dict['label']\n",
    "        if kache_lbl_idx.get('scalabel_label_id', None):\n",
    "            if kache_lbl_idx['scalabel_label_id'] == lbl_idx:\n",
    "                kache_lbl_idx['category'] = scalabel_lbl['categoryPath']\n",
    "                kache_lbl_idx['label_status'] = 30\n",
    "                if not kache_lbl_idx.get('box2d', None):\n",
    "                    kache_lbl_idx['box2d'] = {'x1': scalabel_lbl['data']['x'],\n",
    "                                              'y1': scalabel_lbl['data']['y'],\n",
    "                                              'x2': scalabel_lbl['data']['x'] + scalabel_lbl['data']['w'] -0.5,\n",
    "                                              'y2': scalabel_lbl['data']['y'] + scalabel_lbl['data']['h'] -0.5,}\n",
    "                else:  \n",
    "                    kache_lbl_idx['box2d']['x1'] = scalabel_lbl['data']['x']\n",
    "                    kache_lbl_idx['box2d']['y1'] = scalabel_lbl['data']['y']\n",
    "                    kache_lbl_idx['box2d']['x2'] = scalabel_lbl['data']['x'] + scalabel_lbl['data']['w'] -0.5\n",
    "                    kache_lbl_idx['box2d']['y2'] = scalabel_lbl['data']['y'] + scalabel_lbl['data']['h'] -0.5\n",
    "\n",
    "                if scalabel_lbl.get('attributes', None):\n",
    "                    kache_lbl_idx['attributes']['occluded'] = scalabel_lbl['attributes'].get('occluded', False) if scalabel_lbl['attributes'].get('occluded', False) else scalabel_lbl['attributes'].get('Occluded', False)\n",
    "                    kache_lbl_idx['attributes']['truncated'] = scalabel_lbl['attributes'].get('truncated', False) if scalabel_lbl['attributes'].get('truncated', False) else scalabel_lbl['attributes'].get('Truncated', False)\n",
    "                    kache_lbl_idx['attributes']['trafficLightColor'] = scalabel_lbl['attributes'].get('trafficLightColor', [0, 'NA']) if scalabel_lbl['attributes'].get('trafficLightColor', False) else scalabel_lbl['attributes'].get('Traffic Light Color', False)\n",
    "                    kache_lbl_idx['attributes']['semi'] = scalabel_lbl['attributes'].get('semi', False) if scalabel_lbl['attributes'].get('semi', False) else scalabel_lbl['attributes'].get('Semi', False)\n",
    "                    kache_lbl_idx['attributes']['low_visibility'] = scalabel_lbl['attributes'].get('low_visibility', False) if scalabel_lbl['attributes'].get('low_visibility', False) else scalabel_lbl['attributes'].get('Low_Visibility', False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import copy\n",
    "import os\n",
    "\n",
    "top_indices = {}\n",
    "\n",
    "# Open original scalabel file, get largest id\n",
    "for sca_file in glob.glob('/home/dean/Desktop/pronto_set/scalabel_files/*.json'):\n",
    "    with open(sca_file) as f:\n",
    "        scalabel_data = json.load(f)\n",
    "    if scalabel_data.get(\"Labels\", None):\n",
    "        original_labels = copy.deepcopy(scalabel_data[\"Labels\"])\n",
    "        top_indices[path_leaf(sca_file)] = scalabel_data[\"Labels\"][-1]['id']\n",
    "        print(f\"Largest {path_leaf(sca_file)} index:\", top_indices[path_leaf(sca_file)] )\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "# Open latest corresponding file Scalabel File\n",
    "for sca_file in glob.glob('/home/dean/Desktop/pronto_set/scalabel_files/results/*.json'):\n",
    "    found_lbls = {}\n",
    "    created_lbls = {}\n",
    "    deleted_lbls = {}\n",
    "    sca_key = path_leaf(sca_file.replace('_results', ''))\n",
    "    with open(sca_file) as f:\n",
    "        scalabel_data = json.load(f)\n",
    "        \n",
    "    tot_idx = 0\n",
    "    # Loop through each label and each index.\n",
    "    for img_data in scalabel_data[\"Task\"][\"items\"]:\n",
    "        if img_data.get(\"labelIds\"):\n",
    "            for lbl_idx in sorted(img_data[\"labelIds\"]):\n",
    "                #print(lbl_idx)\n",
    "                scalabel_label = [lbl for lbl in scalabel_data[\"Labels\"] if lbl['id'] == lbl_idx][0]\n",
    "\n",
    "                if lbl_idx == tot_idx: # Found match\n",
    "                    found_lbls[lbl_idx]= { 'url': img_data['url'], 'frame_idx': img_data['index'], 'label': copy.deepcopy(scalabel_label) }\n",
    "                # If the number is greater than the largest index, push to created stack\n",
    "                elif lbl_idx > top_indices[sca_key]: # Created label\n",
    "                    created_lbls[lbl_idx] = { 'url': img_data['url'], 'frame_idx': img_data['index'], 'label': copy.deepcopy(scalabel_label) }\n",
    "                # If the number is less than largest index but more than 1 from previous index, record missing index as deleted\n",
    "                else: # Deleted labels discovered\n",
    "                    #print(f'Missing labels: lbl_idx:{lbl_idx}, tot_idx:{tot_idx}')\n",
    "                    # Add current label to found\n",
    "                    found_lbls[lbl_idx]= { 'url': img_data['url'], 'frame_idx': img_data['index'], 'label': copy.deepcopy(scalabel_label) }\n",
    "                    while tot_idx < lbl_idx:\n",
    "                        #print(f'Appending label to delete lbl_idx:{tot_idx}')\n",
    "                        deleted_lbls[tot_idx]= { 'url': img_data['url'], 'frame_idx': img_data['index'], 'label': copy.deepcopy(scalabel_label) }\n",
    "                        tot_idx += 1\n",
    "                    \n",
    "\n",
    "                tot_idx += 1\n",
    "                \n",
    "                \n",
    "    for i in range(top_indices[sca_key]):\n",
    "        if i not in found_lbls.keys() and i not in created_lbls.keys() and i not in deleted_lbls.keys():\n",
    "            del_lbl = [lbl for lbl in original_labels if lbl['id'] == i]\n",
    "            if del_lbl:\n",
    "                deleted_lbls[i] = { 'url': None, 'frame_idx': None, 'label': copy.deepcopy(del_lbl[0]) }\n",
    "            \n",
    "    print(f\"http://ec2-34-220-1-2.us-west-2.compute.amazonaws.com:8686/label2d?project_name={path_leaf(sca_file.replace('_results.json',''))}&task_index=0\")\n",
    "    print(\"total Altered Labels\", len(set(found_lbls)))\n",
    "    print(\"total Created Labels\", len(set(created_lbls)))\n",
    "    print(\"total Deleted Labels\", len(set(deleted_lbls)))\n",
    "    print(\"Total Labels for Payment Processing: \", len(found_lbls) + len(deleted_lbls))\n",
    "    \n",
    "    ## Update Kache Labels ##\n",
    "    kache_file = f\"/home/dean/Desktop/kache_files/{sca_key}\"\n",
    "    print(kache_file)\n",
    "    if os.path.exists(kache_file):\n",
    "        with open(kache_file) as f:\n",
    "            kache_data = json.load(f)\n",
    "        tot_del, tot_match = 0, 0\n",
    "        \n",
    "        for img_data in kache_data:\n",
    "            if img_data.get(\"labels\", None):\n",
    "                for kache_lbl_idx in img_data[\"labels\"]:\n",
    "                    # Find corresponding labels\n",
    "                    fnd, lbl = found_deleted_label(kache_lbl_idx, deleted_lbls)\n",
    "                    if fnd:\n",
    "                        tot_del +=1\n",
    "                        kache_lbl_idx['label_status'] = 31\n",
    "                        #print(f\"Found deleted label {kache_lbl_idx['scalabel_label_id']} for kache label {kache_lbl_idx['kache_label_id']}\")\n",
    "                    elif kache_lbl_idx['scalabel_label_id'] in found_lbls.keys():\n",
    "                        update_kache_label(kache_lbl_idx, found_lbls)\n",
    "                        #print(f\"Found matching label {kache_lbl_idx['scalabel_label_id']} for kache label {kache_lbl_idx['kache_label_id']}\")\n",
    "                        tot_match +=1\n",
    "                    elif kache_lbl_idx['scalabel_label_id'] in created_lbls.keys(): \n",
    "                        print(f\"Found created label {kache_lbl_idx['scalabel_label_id']} for kache label {kache_lbl_idx['kache_label_id']}\")\n",
    "        print(f\"Total match: {tot_match}, Total deleted: {tot_del}\")\n",
    "                              \n",
    "    \n",
    "    fname = os.path.splitext(path_leaf(sca_file))[0]\n",
    "    with open(f\"/home/dean/Desktop/complete_kache_files/{fname}.json\", \"w+\") as f:\n",
    "        json.dump(list(kache_data), f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipelineapi import PipelineAPI\n",
    "import os\n",
    "\n",
    "def dataset_import(dataset_name, dataset_file):\n",
    "        data_pipeline_api = PipelineAPI(\"http://ec2-54-184-229-238.us-west-2.compute.amazonaws.com/\", 'admin@pronto.ai', 'admin123!')\n",
    "        with open(dataset_file, 'rb') as f:\n",
    "            data_pipeline_api.upload_json_file(f, file_name=path_leaf(dataset_file), round='precision', dataset_name=dataset_name, skip_image=False)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import copy\n",
    "\n",
    "precision_dir = '/home/dean/Desktop/complete_kache_files/precision/'\n",
    "os.makedirs(precision_dir, exist_ok = True)\n",
    "\n",
    "for file in glob.glob('/media/dean/datastore/nebula/pipelines/kache/redshift/precision/labeled_precision_projects/*.json'):\n",
    "    with open(file) as f:\n",
    "        precision_data = json.load(f)\n",
    "    kachedb_datasets = {}\n",
    "    for kachedb_frame_dict in precision_data:\n",
    "\n",
    "       \n",
    "        for lbl in kachedb_frame_dict['labels']:\n",
    "            if lbl['label_status'] == 32:\n",
    "                lbl['label_status'] = 30\n",
    "        \n",
    "        if kachedb_frame_dict['dataset_name'] in kachedb_datasets.keys():\n",
    "            kachedb_datasets[kachedb_frame_dict['dataset_name']].append(copy.deepcopy(kachedb_frame_dict))\n",
    "        else:\n",
    "            kachedb_datasets[kachedb_frame_dict['dataset_name']] = [copy.deepcopy(kachedb_frame_dict)]\n",
    "\n",
    "    for dataset_name, frames in kachedb_datasets.items():\n",
    "        ## Save Labeled Precision Json Files ##\n",
    "        d = datetime.datetime.utcnow()\n",
    "        epoch = datetime.datetime(1970,1,1)\n",
    "        ts = int((d - epoch).total_seconds())\n",
    "        labeled_kachedb_annotations_file = os.path.join(\n",
    "            precision_dir, f\"labeled_{dataset_name}--{ts}.json\")\n",
    "        with open(f\"{labeled_kachedb_annotations_file}\", 'w+') as fn:\n",
    "            json.dump(frames, fn)\n",
    "            \n",
    "        dataset_import(dataset_name, labeled_kachedb_annotations_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_import(\"bdd_set\", '/home/dean/Desktop/complete_kache_files/precision/labeled_bdd_set--1564199177.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "\n",
    "os.makedirs('/home/dean/Desktop/complete_kache_files/', exist_ok = True)\n",
    "for kfile in glob.glob('/home/dean/Desktop/kache_files/*.json'):\n",
    "    with open(kfile) as f:\n",
    "        kache_data = json.load(f)\n",
    "    # Get corresponding scalabel urls\n",
    "    project_name = path_leaf(kfile.replace('_kache_data.json', ''))\n",
    "    # Find corresponding scalabel label id from scalabel files\n",
    "    with open(f'/home/dean/Desktop/pronto_set/scalabel_files/{project_name}.json') as f:\n",
    "        scalabel_data = json.load(f)\n",
    "    \n",
    "    for url, kache_img_data in kache_data.items():        \n",
    "        scalabel_lbl_maches = {}\n",
    "        for scalabel_img_data in scalabel_data[\"Task\"][\"items\"]:\n",
    "            if scalabel_img_data['url'] == url: # found corresponding frame\n",
    "                #print(f\"Found correspoding frame: {project_name}|{url}|{kache_img_data['kache_id']}\")\n",
    "                kache_img_data['scalabel_id'] = scalabel_img_data['index']\n",
    "                kache_img_data['index'] = scalabel_img_data['index']\n",
    "                \n",
    "                # Update scalabel_label_ids for each kache_label_id\n",
    "                if kache_img_data.get('labels', None):\n",
    "                    for kache_idx, kache_label in enumerate(kache_img_data['labels']):\n",
    "                        found = False\n",
    "                      \n",
    "                          \n",
    "                        if scalabel_img_data.get('labelIds', None):\n",
    "                      \n",
    "                            for scalabel_idx in scalabel_img_data['labelIds']:\n",
    "                                scalabel_label = [lbl for lbl in scalabel_data[\"Labels\"] if lbl['id'] == scalabel_idx][0]\n",
    "                                \n",
    "                                if kache_label.get('box2d', None) and \\\n",
    "                                    kache_label['box2d']['x1'] == scalabel_label['data']['x']  and \\\n",
    "                                    kache_label['box2d']['y1'] == scalabel_label['data']['y']:\n",
    "#                                     print('Found match for kache label:', kache_label['kache_label_id'])\n",
    "#                                     print(\"KACHE DATA:\", kache_label)\n",
    "#                                     print(\"SCALABEL DATA:\", scalabel_label)\n",
    "\n",
    "                                    # Found matching label \n",
    "                                    found = True\n",
    "                                    kache_label['scalabel_label_id'] = scalabel_label['id']\n",
    "                                    scalabel_lbl_maches[kache_label['kache_label_id']] = scalabel_label['id']\n",
    "                                    break\n",
    "                                \n",
    "                    # Assign nonmatches to crresponding scalabel label with matching category\n",
    "                    all_lbls = [lbl['kache_label_id'] for lbl in kache_img_data['labels']]\n",
    "                    if list(scalabel_lbl_maches.keys()):\n",
    "                        print('MATCHES:', list(scalabel_lbl_maches.keys()))\n",
    "                    nonmatches = set(all_lbls) - set(list(scalabel_lbl_maches.keys()))\n",
    "                    #print('NONMATCHES:', nonmatches)\n",
    "                    \n",
    "                    for kache_idx, kache_label in enumerate(kache_img_data['labels']):\n",
    "                        if kache_label['kache_label_id'] in nonmatches:\n",
    "                            if scalabel_img_data.get('labelIds', None):\n",
    "                      \n",
    "                                for scalabel_idx in scalabel_img_data['labelIds']:\n",
    "                                    scalabel_label = [lbl for lbl in scalabel_data[\"Labels\"] if lbl['id'] == scalabel_idx][0]\n",
    "\n",
    "\n",
    "                                    # Pair remaining labels with corresponding category\n",
    "                                    if scalabel_label['id'] not in scalabel_lbl_maches.values():\n",
    "\n",
    "                                        if kache_label['category'] == scalabel_label['categoryPath']:\n",
    "                                            # Sync ids for importing changes later\n",
    "                                            kache_label['scalabel_label_id'] = scalabel_label['id']\n",
    "                                            scalabel_lbl_maches[kache_label['kache_label_id']] = scalabel_label['id']\n",
    "                                            # Update nonmatches\n",
    "                                            nonmatches = set(all_lbls) - set(list(scalabel_lbl_maches.keys()))\n",
    "                                            break\n",
    "    \n",
    "    with open(f\"/home/dean/Desktop/complete_kache_files/{project_name}.json\", \"w+\") as f:\n",
    "        json.dump(list(kache_data.values()), f)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import glob\n",
    "# import json\n",
    "# import os\n",
    "\n",
    "# os.makedirs('/home/dean/Desktop/complete_kache_files/', exist_ok = True)\n",
    "# for kfile in glob.glob('/home/dean/Desktop/kache_files/*.json'):\n",
    "#     with open(kfile) as f:\n",
    "#         kache_data = json.load(f)\n",
    "#     # Get corresponding scalabel urls\n",
    "#     project_name = path_leaf(kfile.replace('_kache_data.json', ''))\n",
    "#     # Find corresponding scalabel label id from scalabel files\n",
    "#     with open(f'/home/dean/Desktop/pronto_set/scalabel_files/{project_name}.json') as f:\n",
    "#         scalabel_data = json.load(f)\n",
    "    \n",
    "#     for url, kache_img_data in kache_data.items():        \n",
    "#         scalabel_lbl_maches = {}\n",
    "#         for scalabel_img_data in scalabel_data[\"Task\"][\"items\"]:\n",
    "#             if scalabel_img_data['url'] == url: # found corresponding frame\n",
    "#                 print(f\"Found correspoding frame: {project_name}|{url}|{kache_img_data['kache_id']}\")\n",
    "#                 kache_img_data['scalabel_id'] = scalabel_img_data['index']\n",
    "#                 kache_img_data['index'] = scalabel_img_data['index']\n",
    "                \n",
    "#                 # Update scalabel_label_ids for each kache_label_id\n",
    "#                 if kache_img_data.get('labels', None):\n",
    "#                     for kache_idx, kache_label in enumerate(kache_img_data['labels']):\n",
    "#                         found = False\n",
    "                      \n",
    "                          \n",
    "#                         if scalabel_img_data.get('labels', None):\n",
    "#                             for scalabel_idx, scalabel_label in enumerate(scalabel_img_data['labels']):\n",
    "#                                 if kache_label.get('box2d', None) and \\\n",
    "#                                     kache_label['box2d']['x1'] == scalabel_label['box2d']['x1']  and \\\n",
    "#                                     kache_label['box2d']['y1'] == scalabel_label['box2d']['y1']  and \\\n",
    "#                                     kache_label['box2d']['x2'] == scalabel_label['box2d']['x2']  and \\\n",
    "#                                     kache_label['box2d']['y2'] == scalabel_label['box2d']['y2']:\n",
    "\n",
    "#                                     # Found matching label \n",
    "#                                     found = True\n",
    "#                                     kache_label['scalabel_label_id'] = scalabel_label['id']\n",
    "#                                     scalabel_lbl_maches[kache_label['kache_label_id']] = scalabel_label['id']\n",
    "#                                     break\n",
    "                                \n",
    "#                     # Assign nonmatches to crresponding scalabel label with matching category\n",
    "#                     all_lbls = [lbl['kache_label_id'] for lbl in kache_img_data['labels']]\n",
    "#                     print('MATCHES:', list(scalabel_lbl_maches.keys()))\n",
    "#                     nonmatches = set(all_lbls) - set(list(scalabel_lbl_maches.keys()))\n",
    "#                     print('NONMATCHES:', nonmatches)\n",
    "                    \n",
    "#                     for kache_idx, kache_label in enumerate(kache_img_data['labels']):\n",
    "#                         if kache_label['kache_label_id'] in nonmatches:\n",
    "#                             if scalabel_img_data.get('labels', None):\n",
    "#                                 for scalabel_idx, scalabel_label in enumerate(scalabel_img_data['labels']):\n",
    "#                                     # Pair remaining labels with corresponding category\n",
    "#                                     if scalabel_label['id'] not in scalabel_lbl_maches.values():\n",
    "\n",
    "#                                         if kache_label['category'] == scalabel_label['category']:\n",
    "#                                             # Sync ids for importing changes later\n",
    "#                                             kache_label['scalabel_label_id'] = scalabel_label['id']\n",
    "#                                             scalabel_lbl_maches[kache_label['kache_label_id']] = scalabel_label['id']\n",
    "#                                             # Update nonmatches\n",
    "#                                             nonmatches = set(all_lbls) - set(list(scalabel_lbl_maches.keys()))\n",
    "#                                             break\n",
    "    \n",
    "#     with open(f\"/home/dean/Desktop/complete_kache_files/{project_name}.json\", \"w+\") as f:\n",
    "#         json.dump(list(kache_data.values()), f)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import ntpath\n",
    "import json\n",
    "import os\n",
    "from pipelineapi import PipelineAPI\n",
    "import datetime\n",
    "\n",
    "def get_dataset():\n",
    "    d = datetime.datetime.utcnow()\n",
    "    epoch = datetime.datetime(1970,1,1)\n",
    "    timestamp = int((d - epoch).total_seconds())\n",
    "    batch_size = 0\n",
    "    batch = 0\n",
    "    batch_anns = []\n",
    "    data_pipeline_api = PipelineAPI(\"http://ec2-54-184-229-238.us-west-2.compute.amazonaws.com/\", 'admin@pronto.ai', 'admin123!')\n",
    "\n",
    "\n",
    "    dataset_names = ['holdout_set', 'pronto_set', 'bdd_set']\n",
    "    kache_anns_dir =  '/media/dean/datastore/label_task_data/'\n",
    "    \n",
    "    for dataset_name in dataset_names:\n",
    "\n",
    "        databunch = data_pipeline_api.data(dataset_names=dataset_name, video_source_names=[], categories_names=[],\n",
    "                        label_statuses=[20], page_size=None, frame_id=None) # returns generator\n",
    "\n",
    "        for anns in databunch:\n",
    "            batch_anns.append(anns)\n",
    "            batch_size += len(anns)\n",
    "            if batch_size % 10000 == 0:\n",
    "                with open(os.path.join(kache_anns_dir,'precision--{}--{}--{}.json'.format(dataset_name, batch, timestamp)), 'w') as fn:\n",
    "                    json.dump(batch_anns, fn)\n",
    "                batch_anns = []\n",
    "                batch += 1\n",
    "\n",
    "        # Save any remaining frames\n",
    "        if len(batch_anns) != 0:\n",
    "            with open(os.path.join(kache_anns_dir,'precision--{}--{}--{}.json'.format(dataset_name, batch, timestamp)), 'w') as fn:\n",
    "                json.dump(batch_anns, fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "all_files = [\n",
    "    '/media/dean/datastore/datasets/071519_flickers/*.json'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "for file in glob.glob('/home/dean/Desktop/071519_flickers/*.json'):\n",
    "    \n",
    "    \n",
    "\n",
    "    dataset_name = 'pronto_set'\n",
    "    print(f'Importing file {file}  to db: {dataset_name}')\n",
    "    \n",
    "    \n",
    "\n",
    "    dataset_import(dataset_name, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "    data_pipeline_api = PipelineAPI(\"http://ec2-54-191-65-138.us-west-2.compute.amazonaws.com/\", 'admin@pronto.ai', 'admin123!')\n",
    "\n",
    "    batch_size = 0\n",
    "    batch = 0\n",
    "    batch_anns = []\n",
    "\n",
    "\n",
    "    dataset_names = 'china_set'\n",
    "    kache_anns_dir =  '/home/dean/Desktop'\n",
    "\n",
    "    databunch = data_pipeline_api.data(dataset_names=dataset_names, video_source_names=[], categories_names=[],\n",
    "                    label_statuses=[20], page_size=None, frame_id=None) # returns generator\n",
    "\n",
    "    for anns in databunch:\n",
    "#             anns = update_traffic_light_cats([anns])\n",
    "        batch_anns.append(anns)\n",
    "        batch_size += len(anns)\n",
    "        if batch_size % 5000 == 0:\n",
    "            with open(os.path.join(kache_anns_dir,f'{dataset_names}_{batch}--rejected.json'), 'w') as fn:\n",
    "                json.dump(batch_anns, fn)\n",
    "            batch_anns = []\n",
    "            batch += 1\n",
    "\n",
    "    # Save any remaining frames\n",
    "    if len(batch_anns) != 0:\n",
    "        with open(os.path.join(kache_anns_dir,f'{dataset_names}_{batch}--rejected.json'), 'w') as fn:\n",
    "            json.dump(batch_anns, fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifast1 = '/media/dean/datastore/nebula/pipelines/kache/models/AQEBFEkYOQOfd7FIg9Z5/data/coco/labels/manifast_1.txt'\n",
    "manifast2 = '/media/dean/datastore/nebula/pipelines/kache/models/AQEBFEkYOQOfd7FIg9Z5/data/coco/labels/manifast_2.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "manifasts = glob.glob(os.path.join('/media/dean/datastore/nebula/pipelines/kache/models/AQEB9tV4ZWVzIz9teLfN/data/coco/labels/train2019/', 'manifast*'))\n",
    "\n",
    "f = open(os.path.join('/media/dean/datastore/nebula/pipelines/kache/models/AQEB9tV4ZWVzIz9teLfN/data/coco/labels/train2019/',\"manifasts.txt\"), \"w\")\n",
    "for manifast in manifasts:\n",
    "    mf = open(manifast, \"r\")\n",
    "    f.write(mf.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
