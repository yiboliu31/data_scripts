{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_model_config(path):\n",
    "    \"\"\"Parses the yolo-v3 layer configuration file and returns module definitions\"\"\"\n",
    "    file = open(path, 'r')\n",
    "    lines = file.read().split('\\n')\n",
    "    lines = [x for x in lines if x and not x.startswith('#')]\n",
    "    lines = [x.rstrip().lstrip() for x in lines] # get rid of fringe whitespaces\n",
    "    module_defs = []\n",
    "    for line in lines:\n",
    "        if line.startswith('['): # This marks the start of a new block\n",
    "            module_defs.append({})\n",
    "            module_defs[-1]['type'] = line[1:-1].rstrip()\n",
    "            if module_defs[-1]['type'] == 'convolutional':\n",
    "                module_defs[-1]['batch_normalize'] = 0\n",
    "        else:\n",
    "            key, value = line.split(\"=\")\n",
    "            value = value.strip()\n",
    "            module_defs[-1][key.rstrip()] = value.strip()\n",
    "\n",
    "    return module_defs\n",
    "\n",
    "\n",
    "def save_model_config(model_defs, path):\n",
    "    \"\"\"Saves the yolo-v3 layer configuration file\"\"\"\n",
    "    with open(path, 'w') as writer:\n",
    "        \n",
    "        for block in model_defs:\n",
    "            writer.write('['+ block['type'] +']'+'\\n')\n",
    "            [writer.write(str(k)+'='+str(v)+'\\n') for k,v in block.items() if k != 'type']\n",
    "            writer.write('\\n')\n",
    "    return path\n",
    "\n",
    "            \n",
    "def save_data_config(data_config, path):\n",
    "    \"\"\"Saves the yolo-v3 data configuration file\"\"\"\n",
    "    with open(path, 'w') as writer:\n",
    "        [writer.write(str(k)+'='+str(v)+'\\n') for k,v in data_config.items()]\n",
    "    return path\n",
    "\n",
    "\n",
    "def inject_model_config(dataset, model_config, hyperparams):\n",
    "    for i, block in enumerate(model_config):        \n",
    "        if block['type'] == 'net':\n",
    "            block['learning_rate'] = hyperparams['lr']\n",
    "            block['batch'] = hyperparams['batch']\n",
    "            block['subdivisions'] = hyperparams['subdivisions']\n",
    "            block['burn_in'] = len(dataset._images.items())//(hyperparams['gpus'] * hyperparams['batch'])\n",
    "            block['max_batches'] = len(dataset._images.items())//(hyperparams['gpus'] * hyperparams['batch']) * hyperparams['epochs']\n",
    "        elif block['type'] == 'yolo':\n",
    "            block['classes'] = len(dataset.category_names)\n",
    "            model_config[i-1]['filters'] = (len(dataset.category_names)+5)*3\n",
    "    return model_config\n",
    "\n",
    "\n",
    "def inject_data_config(dataset, data_config):\n",
    "    data_config['train'] = dataset.darknet_manifast\n",
    "    data_config['classes'] = len(dataset.category_names)\n",
    "    ## TODO: Add Validation Set\n",
    "    data_config['valid'] = dataset.darknet_manifast\n",
    "    data_config['names'] = dataset.names_config\n",
    "    backup_path = os.path.abspath(os.path.join(dataset.output_path, os.pardir, 'backup'))\n",
    "    os.makedirs(backup_path, exist_ok = True)\n",
    "    data_config['backup'] = os.path.abspath(os.path.join(dataset.output_path, os.pardir, 'backup'))\n",
    "    num_gpus = int(dataset.parse_nvidia_smi()['Attached GPUs'])\n",
    "    data_config['gpus'] = ','.join(str(i) for i in range(num_gpus))\n",
    "    \n",
    "    \n",
    "    return data_config\n",
    "            \n",
    "\n",
    "def parse_data_config(path):\n",
    "    \"\"\"Parses the data configuration file\"\"\"\n",
    "    options = dict()\n",
    "    options['gpus'] = '0,1'\n",
    "    with open(path, 'r') as fp:\n",
    "        lines = fp.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line == '' or line.startswith('#'):\n",
    "            continue\n",
    "        key, value = line.split('=')\n",
    "        options[key.strip()] = value.strip()\n",
    "    return options\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import urllib\n",
    "from PIL import Image\n",
    "from enum import Enum\n",
    "from pycocotools.coco import COCO\n",
    "import xml.etree.cElementTree as ET\n",
    "import glob\n",
    "import argparse\n",
    "import numpy as np\n",
    "import json\n",
    "import numpy\n",
    "import cv2\n",
    "from collections import OrderedDict\n",
    "import scipy.misc\n",
    "from skimage import measure   \n",
    "import random\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import shutil\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from subprocess import Popen,PIPE,STDOUT,call \n",
    "\n",
    "\n",
    "from utils import datasets\n",
    "\n",
    "BDD100K_DIRECTORY = os.path.join('/media/dean/datastore1/datasets/BerkeleyDeepDrive', 'bdd100k')\n",
    "WORKING_DIRECTORY = '/media/dean/datastore1/datasets/darknet'\n",
    "DATA_DIRECTORY = os.path.join(WORKING_DIRECTORY, 'data')\n",
    "COCO_DIRECTORY = os.path.join(DATA_DIRECTORY, 'coco')\n",
    "\n",
    "TRAINERS_DIRECTORY = os.path.join(WORKING_DIRECTORY, 'trainers')\n",
    "ANNOTATIONS_FILE = os.path.join(BDD100K_DIRECTORY, 'labels/bdd100k_labels_images_val.json')\n",
    "\n",
    "BASE_DATA_CONFIG = os.path.join(WORKING_DIRECTORY, 'cfg', 'bdd100k.data')\n",
    "BASE_MODEL_CONFIG = os.path.join(WORKING_DIRECTORY, 'cfg', 'yolov3-bdd100k.cfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of COCO Images 10000\n",
      "Length of Coco Annotations: 185526\n",
      "Initiating Trainer: /media/dean/datastore1/datasets/darknet/trainers/run6_1gpu_001lr_64bat_16sd_600ep\n",
      "{'gpus': '0,1', 'classes': '10', 'train': '/media/anthony/deans_data/darknet/data/coco/labels/train2014/manifast.txt', 'valid': '/media/anthony/deans_data/darknet/data/coco/labels/train2014/image_list.txt', 'names': '/media/anthony/deans_data/darknet/data/coco.bdd100k.names', 'backup': '/media/anthony/deans_data/darknet/backup', 'eval': 'coco'} \n",
      "\n",
      "{'gpus': '0', 'classes': 12, 'train': '/media/dean/datastore1/datasets/darknet/trainers/run6_1gpu_001lr_64bat_16sd_600ep/data/coco/labels/val2014/manifast.txt', 'valid': '/media/dean/datastore1/datasets/darknet/trainers/run6_1gpu_001lr_64bat_16sd_600ep/data/coco/labels/val2014/manifast.txt', 'names': '/media/dean/datastore1/datasets/darknet/trainers/run6_1gpu_001lr_64bat_16sd_600ep/cfg/COCO_val2014_0000.names', 'backup': '/media/dean/datastore1/datasets/darknet/trainers/run6_1gpu_001lr_64bat_16sd_600ep/backup', 'eval': 'coco'} \n",
      "\n",
      "Initializing Training with the following parameters: \n",
      " cd /media/dean/datastore1/datasets/darknet && ./darknet detector train /media/dean/datastore1/datasets/darknet/trainers/run6_1gpu_001lr_64bat_16sd_600ep/cfg/bdd100k.data /media/dean/datastore1/datasets/darknet/trainers/run6_1gpu_001lr_64bat_16sd_600ep/cfg/yolov3-bdd100k.cfg /media/dean/datastore1/datasets/darknet/trainers/run6_1gpu_001lr_64bat_16sd_600ep/darknet53.conv.74 -gpus 0 | tee -a /media/dean/datastore1/datasets/darknet/trainers/run6_1gpu_001lr_64bat_16sd_600ep/training_results.txt\n"
     ]
    }
   ],
   "source": [
    "# For Run in Training Runs    \n",
    "trainers = os.listdir(TRAINERS_DIRECTORY)\n",
    "all_training_runs = []\n",
    "\n",
    "for trainer in trainers:\n",
    "    ## Prepare Dataset ##\n",
    "    bdd_set = datasets.DataFormatter(annotations_list = ANNOTATIONS_FILE, input_format = datasets.Format.bdd,\n",
    "                                     #pickle_file = 'BerkeleyDeepDrive_bdd100k_labels_kache_bdd100k_labels_images_train.json.pickle',\n",
    "                                     output_path = os.path.join(TRAINERS_DIRECTORY,trainer,'data'),\n",
    "                                     trainer_prefix = 'COCO_val2014_0000', \n",
    "                                     s3_bucket = 'kache-scalabel/bdd100k/images/100k/val/')\n",
    "    bdd_set.export(datasets.Format.darknet)\n",
    "\n",
    "    print('Initiating Trainer:', os.path.join(TRAINERS_DIRECTORY,trainer))\n",
    "    # Grab hyperparameters from filename\n",
    "    tokens = trainer.split('_')\n",
    "    hyperparams = {'name': tokens[0], \n",
    "                   'gpus': int(tokens[1].replace('gpu','')),\n",
    "                   'lr': float('0.'+tokens[2].replace('lr','')),\n",
    "                   'batch': int(tokens[3].replace('bat','')),\n",
    "                   'subdivisions': int(tokens[4].replace('sd','')),\n",
    "                   'epochs': int(tokens[5].replace('ep',''))}\n",
    "    \n",
    "    \n",
    "    # Update data config\n",
    "    data_config = parse_data_config(BASE_DATA_CONFIG)\n",
    "    print(data_config,'\\n')\n",
    "    os.makedirs(os.path.join(TRAINERS_DIRECTORY,trainer, 'data'), exist_ok = True)\n",
    "    data_config = inject_data_config(bdd_set, data_config)\n",
    "    print(data_config,'\\n')\n",
    "    data_cfg_path = save_data_config(data_config, os.path.join(TRAINERS_DIRECTORY,trainer, 'cfg', os.path.split(BASE_DATA_CONFIG)[-1]))\n",
    "    \n",
    "    # Update model config\n",
    "    model_config = parse_model_config(BASE_MODEL_CONFIG)\n",
    "    model_config = inject_model_config(bdd_set, model_config, hyperparams)\n",
    "    os.makedirs(os.path.join(TRAINERS_DIRECTORY,trainer, 'cfg'), exist_ok = True)\n",
    "    model_cfg_path = save_model_config(model_config, os.path.join(TRAINERS_DIRECTORY,trainer, 'cfg', os.path.split(BASE_MODEL_CONFIG)[-1]))\n",
    "    \n",
    "    ##TODO: Download Darknet 53 weights into backup folder\n",
    "    \n",
    "    ## Run Training ##\n",
    "    TRAINER_DIR = os.path.join(TRAINERS_DIRECTORY,trainer)\n",
    "    train_results_file = os.path.join(TRAINER_DIR, 'training_results.txt')\n",
    "    CURRENT_WEIGHT = os.path.join(TRAINERS_DIRECTORY,trainer, 'darknet53.conv.74')\n",
    "    num_gpus = data_config['gpus']\n",
    "    darknet_train = \"cd {} && ./darknet detector train {} {} {} -gpus {} | tee -a {}\".format(WORKING_DIRECTORY,\n",
    "                                data_cfg_path, model_cfg_path, CURRENT_WEIGHT, num_gpus, train_results_file)\n",
    "    print('Initializing Training with the following parameters:','\\n', darknet_train)\n",
    "    proc=Popen(darknet_train, shell=True, stdout=PIPE)\n",
    "\n",
    "    with open(train_results_file+'.backup',\"w+\") as f:\n",
    "        f.write(proc.communicate()[0].decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdd_set.export(datasets.Format.scalabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
