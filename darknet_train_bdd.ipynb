{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run Evaluation Script\n",
    "# %matplotlib inline\n",
    "# %run /media/dean/datastore1/datasets/Scripts/darknet_evaluate_bdd.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_model_config(path):\n",
    "    \"\"\"Parses the yolo-v3 layer configuration file and returns module definitions\"\"\"\n",
    "    file = open(path, 'r')\n",
    "    lines = file.read().split('\\n')\n",
    "    lines = [x for x in lines if x and not x.startswith('#')]\n",
    "    lines = [x.rstrip().lstrip() for x in lines] # get rid of fringe whitespaces\n",
    "    module_defs = []\n",
    "    for line in lines:\n",
    "        if line.startswith('['): # This marks the start of a new block\n",
    "            module_defs.append({})\n",
    "            module_defs[-1]['type'] = line[1:-1].rstrip()\n",
    "            if module_defs[-1]['type'] == 'convolutional':\n",
    "                module_defs[-1]['batch_normalize'] = 0\n",
    "        else:\n",
    "            key, value = line.split(\"=\")\n",
    "            value = value.strip()\n",
    "            module_defs[-1][key.rstrip()] = value.strip()\n",
    "\n",
    "    return module_defs\n",
    "\n",
    "\n",
    "def save_model_config(model_defs, path):\n",
    "    \"\"\"Saves the yolo-v3 layer configuration file\"\"\"\n",
    "    with open(path, 'w') as writer:\n",
    "        \n",
    "        for block in model_defs:\n",
    "            writer.write('['+ block['type'] +']'+'\\n')\n",
    "            [writer.write(str(k)+'='+str(v)+'\\n') for k,v in block.items() if k != 'type']\n",
    "            writer.write('\\n')\n",
    "    \n",
    "\n",
    "def parse_data_config(path):\n",
    "    \"\"\"Parses the data configuration file\"\"\"\n",
    "    options = dict()\n",
    "    options['gpus'] = '0,1'\n",
    "    with open(path, 'r') as fp:\n",
    "        lines = fp.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line == '' or line.startswith('#'):\n",
    "            continue\n",
    "        key, value = line.split('=')\n",
    "        options[key.strip()] = value.strip()\n",
    "    return options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import urllib\n",
    "from PIL import Image\n",
    "from enum import Enum\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "\n",
    "import xml.etree.cElementTree as ET\n",
    "import glob\n",
    "import argparse\n",
    "import numpy as np\n",
    "import json\n",
    "import numpy\n",
    "import cv2\n",
    "from collections import OrderedDict\n",
    "import scipy.misc\n",
    "from skimage import measure   \n",
    "import random\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import shutil\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from utils import datasets\n",
    "\n",
    "WORKING_DIRECTORY = '/media/dean/datastore1/datasets/darknet'\n",
    "DATA_DIRECTORY = os.path.join(WORKING_DIRECTORY, 'data')\n",
    "BDD100K_DIRECTORY = os.path.join('/media/dean/datastore1/datasets/BerkeleyDeepDrive', 'bdd100k')\n",
    "COCO_DIRECTORY = os.path.join(DATA_DIRECTORY, 'coco')\n",
    "BDD100K_COCO_ANNOTATIONS_FILE = os.path.join(COCO_DIRECTORY,'annotations' , 'bdd100k_instances_train2014.json')\n",
    "\n",
    "\n",
    "\n",
    "CATEGORY_NAMES = os.path.join(WORKING_DIRECTORY, 'data', 'coco.bdd100k.names')\n",
    "TRAINERS_DIRECTORY = os.path.join(WORKING_DIRECTORY, 'trainers')\n",
    "ANNOTATIONS_FILE = os.path.join(BDD100K_DIRECTORY, 'labels/kache_bdd100k_labels_images_train.json', )\n",
    "\n",
    "DATA_CONFIG_PATH = os.path.join(WORKING_DIRECTORY, 'cfg', 'bdd100k.data')\n",
    "MODEL_CONFIG_PATH = os.path.join(WORKING_DIRECTORY, 'cfg', 'yolov3-bdd100k.cfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69863\n",
      "Length of Coco Annotations: 1286871\n",
      "Initiating Trainer: /media/dean/datastore1/datasets/darknet/trainers/run6_2gpu_001lr_256bat_64sd_60ep\n",
      "\t Hyperparameters: {'name': 'run6', 'gpus': 2, 'lr': 0.001, 'batch': 256, 'subdivisions': 64, 'epochs': 60}\n",
      "{'gpus': '0,1', 'classes': '10', 'train': '/media/anthony/deans_data/darknet/data/coco/labels/train2014/manifast.txt', 'valid': '/media/anthony/deans_data/darknet/data/coco/labels/train2014/image_list.txt', 'names': '/media/anthony/deans_data/darknet/data/coco.bdd100k.names', 'backup': '/media/anthony/deans_data/darknet/backup', 'eval': 'coco'}\n"
     ]
    }
   ],
   "source": [
    "# For Run in Training Runs    \n",
    "trainers = os.listdir(TRAINERS_DIRECTORY)\n",
    "all_training_runs = []\n",
    "\n",
    "for trainer in trainers:\n",
    "    ## Prepare Dataset ##\n",
    "    bdd_set = datasets.DataFormatter(annotations_list = ANNOTATIONS_FILE, data_format = datasets.Format.bdd, \n",
    "                                     output_path = os.path.join(TRAINERS_DIRECTORY,trainer,'data'),\n",
    "                                     trainer_prefix = 'COCO_train2014_0000', s3_bucket = 'kache-scalabel/bdd100k/images/100k/train/')\n",
    "#     print(list(bdd_set._images.values())[:5],\"\\n\"*4)\n",
    "    \n",
    "    # TODO: Export Dataset to darknet format\n",
    "    bdd_set.export(datasets.Format.darknet)\n",
    "    \n",
    "    \n",
    "    # TODO: Generate data config in trainers directory.\n",
    "    \n",
    "    print('Initiating Trainer:', os.path.join(TRAINERS_DIRECTORY,trainer))\n",
    "    # Grab hyperparameters from filename\n",
    "    tokens = trainer.split('_')\n",
    "    hyperparams = {'name': tokens[0], \n",
    "                   'gpus': int(tokens[1].replace('gpu','')),\n",
    "                   'lr': float('0.'+tokens[2].replace('lr','')),\n",
    "                   'batch': int(tokens[3].replace('bat','')),\n",
    "                   'subdivisions': int(tokens[4].replace('sd','')),\n",
    "                   'epochs': int(tokens[5].replace('ep',''))}\n",
    "    print('\\t', 'Hyperparameters:', hyperparams)\n",
    "    \n",
    "    # Update model config\n",
    "    \n",
    "    model_config = parse_model_config(MODEL_CONFIG_PATH)\n",
    "    for block in model_config:        \n",
    "        if block['type'] == 'net':\n",
    "            block['learning_rate'] = hyperparams['lr']\n",
    "            block['batch'] = hyperparams['batch']\n",
    "            block['subdivisions'] = hyperparams['subdivisions']\n",
    "            block['burn_in'] = len(bdd_set._images)//(hyperparams['gpus'] * hyperparams['batch'])\n",
    "            block['max_batches'] = len(bdd_set._images)//(hyperparams['gpus'] * hyperparams['batch']) * hyperparams['epochs']\n",
    "\n",
    "    os.makedirs(os.path.join(TRAINERS_DIRECTORY,trainer, 'cfg'), exist_ok = True)\n",
    "    save_model_config(model_config, os.path.join(TRAINERS_DIRECTORY,trainer, 'cfg', os.path.split(MODEL_CONFIG_PATH)[-1]))\n",
    "    \n",
    "    # Update data config\n",
    "    data_config = parse_data_config(DATA_CONFIG_PATH)\n",
    "    print(data_config)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bdd_set.export(format = datasets.Format.darknet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
